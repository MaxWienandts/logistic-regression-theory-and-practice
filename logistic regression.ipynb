{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92553f42",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "*From‑Scratch & scikit‑learn implementation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c1d7f-b727-43b8-89bc-ea35baa67355",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "## Why Logistic Regression?\n",
    "\n",
    "Logistic regression is the workhorse of **binary classification**—the task of predicting whether an observation belongs to class 0 or class 1.  \n",
    "Despite its name, it is **not** a regression algorithm for continuous targets; rather, it models the *probability* of class membership using a logistic (sigmoid) curve.\n",
    "\n",
    "* **Intuition**  \n",
    "  - Linear regression lets predicted values wander from $-\\infty$ to $+\\infty$.  \n",
    "  - Classification, however, demands outputs bounded in $[0,1]$ so they can be interpreted as probabilities.  \n",
    "  - Logistic regression achieves this by passing a linear combination of features through the sigmoid function:\n",
    "    $$\n",
    "      \\hat{p} = \\sigma(z) = \\frac{1}{1 + e^{-z}},\\quad \n",
    "      \\text{where } z = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_kx_k.\n",
    "    $$\n",
    "\n",
    "* **Why start here?**  \n",
    "  - It is **interpretable**: coefficients translate into *odds ratios* that non-technical stakeholders grasp.  \n",
    "  - It scales gracefully from a single predictor to thousands of sparse features.  \n",
    "  - Many modern classifiers (neural nets, transformers) still end with a logistic layer for binary outputs.\n",
    "\n",
    "\n",
    "* **What you’ll learn in this notebook**  \n",
    "  1. The mathematical foundations: likelihood, log-odds, and cross-entropy loss.  \n",
    "  2. A from-scratch NumPy implementation using gradient descent.  \n",
    "  3. A production-ready approach with `scikit-learn`.\n",
    "  4. Practical tips: handling class imbalance, feature scaling, regularisation, and model interpretation.\n",
    "\n",
    "By mastering logistic regression, you build a solid springboard to more advanced classification techniques while retaining a firm grasp on the underlying statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79fd4e0",
   "metadata": {},
   "source": [
    "## When to Use Linear vs. Logistic Regression\n",
    "\n",
    "| Scenario | **Linear Regression** | **Logistic Regression** |\n",
    "|---|---|---|\n",
    "| **Nature of the target** | Continuous, unbounded numeric outcome (e.g., revenue, temperature) | Categorical – most often binary—outcome (e.g., churn yes/no, disease present/absent) |\n",
    "| **Goal** | Estimate the conditional mean of *y* given *X* | Estimate $P(y=1 \\mid X)$ and/or assign class labels |\n",
    "| **Core likelihood / assumptions** | Gaussian errors, constant variance, linear mean function | Binomial likelihood; log-odds are a linear function of predictors |\n",
    "| **Coefficient interpretation** | Slope = expected change in *y* per unit change in *x* | Coefficient = change in log-odds (or multiplicative change in the odds ratio) per unit change in *x* |\n",
    "| **Typical evaluation metrics** | RMSE, MAE, $R^{2}$ | Accuracy, Precision-Recall, ROC-AUC, Log-loss |\n",
    "| **Common pitfalls** | Produces impossible values for bounded targets; sensitive to heteroscedasticity | Misleading accuracy on imbalanced classes; assumes a linear decision boundary |\n",
    "| **Example use cases** | Forecasting sales, predicting house prices | Fraud detection, spam classification, medical diagnosis |\n",
    "\n",
    "**Rule of thumb**\n",
    "\n",
    "* Use **linear regression** when the dependent variable is numeric and unbounded and you care about predicting its magnitude.  \n",
    "* Use **logistic regression** when the dependent variable represents category membership (especially a yes/no outcome) and you need well-calibrated probabilities confined to the $[0,1]$ interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ab255a2-b1bc-410b-9560-084ce0a2eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3149d16",
   "metadata": {},
   "source": [
    "## Loading and Exploring the Data\n",
    "\n",
    "### Wisconsin Diagnostic Breast Cancer (WDBC) Dataset — Overview\n",
    "\n",
    "| Aspect | Details |\n",
    "|--------|---------|\n",
    "| **Origin** | Collected at the University of Wisconsin–Madison; archived on the UCI Machine Learning Repository. |\n",
    "| **Objective** | Distinguish **malignant** from **benign** breast‐tumor samples based on features computed from digitized images of fine-needle aspirate (FNA) biopsies. |\n",
    "| **Observations** | **569** cases (rows). |\n",
    "| **Target variable** | `target` – binary label (0 = malignant, 1 = benign). |\n",
    "| **Class balance** | 212 malignant (≈ 37 %), 357 benign (≈ 63 %). |\n",
    "| **Predictor variables** | 30 continuous attributes describing cell‐nucleus morphology; for each of 10 raw measurements — `radius`, `texture`, `perimeter`, `area`, `smoothness`, `compactness`, `concavity`, `concave points`, `symmetry`, `fractal_dimension` — three summary statistics are recorded: <br> • **mean** over all nuclei in the image <br> • **standard error (SE)** <br> • **“worst”/max** value |\n",
    "| **Feature scale & units** | All features are real-valued and on heterogeneous scales (e.g., radius in pixels, area in pixel²). Standardisation is recommended before model fitting. |\n",
    "| **Missing values** | None; the matrix is complete. |\n",
    "| **Data format in `sklearn`** | `load_breast_cancer(as_frame=True)` returns: <br> • `.frame` → `pandas.DataFrame` with the 30 features **plus** the `target` column. <br> • `.target_names` → `['malignant', 'benign']` <br> • `.feature_names` → ordered list of the 30 predictor names <br> • `.DESCR` → full text description. |\n",
    "| **Typical ML usage** | Benchmark for binary classification algorithms, feature‐selection experiments, model interpretability studies, and demonstrations of class-imbalance handling. |\n",
    "\n",
    "**Key point**: Each feature captures a different aspect of cell morphology; malignant tumours generally exhibit larger, more irregular, and less symmetrical nuclei, which is why models trained on this dataset often achieve high discriminative performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31065e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f13e1-8803-4d6d-8bc4-c1b6276e7171",
   "metadata": {},
   "source": [
    "## Initial Checks\n",
    "I am not focusing on exploratory data analysis (EDA) in this notebook. However, I will perform some basic data checks to identify any potential issues with the dataset.\n",
    "\n",
    "Specifically, I want to verify:\n",
    "- that all data types are correct (e.g., no unexpected characters in numeric variables),\n",
    "- that there are no missing values,\n",
    "- and that the variables exhibit some variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f2c1b9-ee14-4843-a366-d482c5abfb9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_values</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         missing_values  percent\n",
       "mean radius                           0      0.0\n",
       "mean texture                          0      0.0\n",
       "mean perimeter                        0      0.0\n",
       "mean area                             0      0.0\n",
       "mean smoothness                       0      0.0\n",
       "mean compactness                      0      0.0\n",
       "mean concavity                        0      0.0\n",
       "mean concave points                   0      0.0\n",
       "mean symmetry                         0      0.0\n",
       "mean fractal dimension                0      0.0\n",
       "radius error                          0      0.0\n",
       "texture error                         0      0.0\n",
       "perimeter error                       0      0.0\n",
       "area error                            0      0.0\n",
       "smoothness error                      0      0.0\n",
       "compactness error                     0      0.0\n",
       "concavity error                       0      0.0\n",
       "concave points error                  0      0.0\n",
       "symmetry error                        0      0.0\n",
       "fractal dimension error               0      0.0\n",
       "worst radius                          0      0.0\n",
       "worst texture                         0      0.0\n",
       "worst perimeter                       0      0.0\n",
       "worst area                            0      0.0\n",
       "worst smoothness                      0      0.0\n",
       "worst compactness                     0      0.0\n",
       "worst concavity                       0      0.0\n",
       "worst concave points                  0      0.0\n",
       "worst symmetry                        0      0.0\n",
       "worst fractal dimension               0      0.0\n",
       "target                                0      0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing values\n",
    "df_missing = pd.DataFrame(df.isna().sum(), columns = ['missing_values'])\n",
    "df_missing['percent'] = df_missing['missing_values'] / len(df)\n",
    "df_missing.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5cc852f-90ef-4b56-8856-24aaff422a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  target                   569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "# Data type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64eb73ed-c837-4a59-8f7d-e6b13b533444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.627417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.483918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst texture  \\\n",
       "count     569.000000              569.000000  ...     569.000000   \n",
       "mean        0.181162                0.062798  ...      25.677223   \n",
       "std         0.027414                0.007060  ...       6.146258   \n",
       "min         0.106000                0.049960  ...      12.020000   \n",
       "25%         0.161900                0.057700  ...      21.080000   \n",
       "50%         0.179200                0.061540  ...      25.410000   \n",
       "75%         0.195700                0.066120  ...      29.720000   \n",
       "max         0.304000                0.097440  ...      49.540000   \n",
       "\n",
       "       worst perimeter   worst area  worst smoothness  worst compactness  \\\n",
       "count       569.000000   569.000000        569.000000         569.000000   \n",
       "mean        107.261213   880.583128          0.132369           0.254265   \n",
       "std          33.602542   569.356993          0.022832           0.157336   \n",
       "min          50.410000   185.200000          0.071170           0.027290   \n",
       "25%          84.110000   515.300000          0.116600           0.147200   \n",
       "50%          97.660000   686.500000          0.131300           0.211900   \n",
       "75%         125.400000  1084.000000          0.146000           0.339100   \n",
       "max         251.200000  4254.000000          0.222600           1.058000   \n",
       "\n",
       "       worst concavity  worst concave points  worst symmetry  \\\n",
       "count       569.000000            569.000000      569.000000   \n",
       "mean          0.272188              0.114606        0.290076   \n",
       "std           0.208624              0.065732        0.061867   \n",
       "min           0.000000              0.000000        0.156500   \n",
       "25%           0.114500              0.064930        0.250400   \n",
       "50%           0.226700              0.099930        0.282200   \n",
       "75%           0.382900              0.161400        0.317900   \n",
       "max           1.252000              0.291000        0.663800   \n",
       "\n",
       "       worst fractal dimension      target  \n",
       "count               569.000000  569.000000  \n",
       "mean                  0.083946    0.627417  \n",
       "std                   0.018061    0.483918  \n",
       "min                   0.055040    0.000000  \n",
       "25%                   0.071460    0.000000  \n",
       "50%                   0.080040    1.000000  \n",
       "75%                   0.092080    1.000000  \n",
       "max                   0.207500    1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd04cc-895a-4663-a95f-0b656fae067c",
   "metadata": {},
   "source": [
    "For this notebook, we are not focused on performance — our goal is simply to understand how logistic regression works.\n",
    "\n",
    "That said, we need to ensure that our dataset contains **exactly two classes** in the target variable, and that there are **sufficient examples of both classes**. \\\n",
    "Otherwise, the model won’t be able to learn meaningful decision boundaries, and the training process might fail or produce misleading results.\n",
    "\n",
    "In practice, this means:\n",
    "\n",
    "- Checking that the target variable is binary (e.g., 0 and 1).\n",
    "- Confirming that both classes are represented with a reasonable number of observations.\n",
    "- Avoiding highly imbalanced datasets, as they can distort interpretation in educational examples.\n",
    "\n",
    "Once we’ve verified class balance, we can safely proceed to fitting and interpreting logistic regression models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86948edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.value_counts(normalize=True).plot(kind='bar');\n",
    "plt.title('Class distribution (0 = malignant, 1 = benign)')\n",
    "plt.ylabel('Proportion')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03af4a1-230a-4f68-ab65-fa30baa59e62",
   "metadata": {},
   "source": [
    "### Data Preparation: Train–Test Split & Feature Scaling\n",
    "\n",
    "Before training the model, we need to:\n",
    "\n",
    "- **Separate features and target**  (Already done!)\n",
    "  - Before fitting a model we must **isolate what we are going to predict** and **prepare the data in a way that avoids information leakage**.\n",
    "  - X contains all explanatory variables.\n",
    "  - y is the binary label (0 = malignant, 1 = benign in the breast-cancer example).\n",
    "\n",
    "- **Create a hold-out set**  \n",
    "  - test_size=0.2 reserves 20 % of the data for final evaluation.\n",
    "  - stratify=y keeps the original class distribution in both splits—crucial for balanced performance metrics on imbalanced datasets.\n",
    "  - random_state guarantees reproducible splits.\n",
    "\n",
    "- **Standardise the features**  \n",
    "  - Many optimisation algorithms (including gradient descent) converge faster when inputs have zero mean and unit variance.\n",
    "  - We fit the scaler only on the training data to avoid peeking at the test set, then transform both splits with the same parameters—this prevents data leakage.\n",
    "\n",
    "- **Verify the result**  \n",
    "  - Verifying the sample counts ensures the split happened as expected and helps catch accidental filtering or indexing errors early.\n",
    "\n",
    "With a clean split and properly scaled features, we are ready to fit logistic-regression models and assess their generalisation performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6b7d4c-a0e0-48ad-874d-f101644eb6f2",
   "metadata": {},
   "source": [
    "## More about Standardise the features\n",
    "\n",
    "### Why Standardising Aids Coefficient Interpretation and Significance Tests\n",
    "\n",
    "Logistic-regression coefficients measure the change in log-odds produced by a one-unit increase in a predictor, but a “unit” can mean wildly different things across variables:\n",
    "\n",
    "| Variable        | Raw unit     | Typical range         |\n",
    "|----------------|--------------|------------------------|\n",
    "| age            | 1 year       | 18 – 90                |\n",
    "| annual_income  | 1 dollar     | 20,000 – 300,000       |\n",
    "| tumour_radius  | 1 millimetre | 6 – 30                 |\n",
    "\n",
    "Because the scales differ by orders of magnitude, the raw coefficients are not directly comparable:\n",
    "\n",
    "- A tiny coefficient on `annual_income` may look unimportant only because dollars are small units.\n",
    "- A large coefficient on `tumour_radius` may look dominant simply because millimetres are coarse.\n",
    "\n",
    "---\n",
    "\n",
    "### How Standardisation Fixes This\n",
    "\n",
    "Standardising transforms every predictor to have mean 0 and standard deviation 1:\n",
    "\n",
    "$$\n",
    "x^* = \\frac{x - \\bar{x}}{s}\n",
    "$$\n",
    "\n",
    "A one-unit change in $ x^* $ now means one standard deviation in the original units.\n",
    "\n",
    "**Coefficients therefore quantify the change in log-odds per equally sized shift across all variables**, making magnitudes directly comparable.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation Shortcut\n",
    "\n",
    "If the standardised coefficient of `tumour_radius` is 0.8 and that of `texture_variance` is 0.2:\n",
    "\n",
    "- A one-SD increase in radius multiplies the odds roughly $ e^{0.8} \\approx 2.2 $ times.\n",
    "- The same relative increase in texture multiplies the odds by only $ e^{0.2} \\approx 1.2 $ times.\n",
    "\n",
    "---\n",
    "\n",
    "### Impact on Statistical Significance\n",
    "\n",
    "*p*-values and *z*-scores depend on the ratio:\n",
    "\n",
    "$$\n",
    "\\frac{\\beta}{SE(\\beta)}\n",
    "$$\n",
    "\n",
    "Standardisation rescales both the numerator (coefficient) and the denominator (standard error) by the same factor, so **significance tests remain mathematically identical**.\n",
    "\n",
    "What changes is our ability to compare effect sizes, because coefficients are now expressed in commensurate units.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "Suppose we model employee attrition using:\n",
    "\n",
    "- `salary_usd`\n",
    "- `years_at_company`\n",
    "- `engagement_score` (0–1)\n",
    "\n",
    "Without scaling, the largest coefficient may attach to `engagement_score` purely because its domain is small. After standardising:\n",
    "\n",
    "- The coefficient for `salary_usd` might grow substantially (one SD ≈ \\$15,000) and reveal that salary has a stronger impact on quit-probability than raw numbers suggested.\n",
    "- `engagement_score`’s coefficient may shrink, clarifying that improving engagement by one SD (≈ 0.12 points) is **less influential** than a comparable relative boost in salary.\n",
    "\n",
    "---\n",
    "\n",
    "### **Takeaway**\n",
    "\n",
    "Standardising features **does not alter statistical significance**, but it **does make coefficient magnitudes, odds ratios, and regularisation penalties directly comparable across predictors** — essential for:\n",
    "\n",
    "- Ranking variable importance  \n",
    "- Communicating results to stakeholders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a496b48-5b37-4a56-8108-19cab759b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train‑test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aea9fc0-7d72-4101-976c-627d06e0fb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 455\n",
      "Test samples: 114\n"
     ]
    }
   ],
   "source": [
    "# Standardise features    \n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print('Training samples:', X_train.shape[0])\n",
    "print('Test samples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aefc45",
   "metadata": {},
   "source": [
    "## Logistic Regression From Scratch — Mathematical Walk-through\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Model Specification\n",
    "\n",
    "For binary outcomes $ y \\in \\{0,1\\} $, we posit:\n",
    "\n",
    "$$\n",
    "\\Pr(y=1 \\mid \\mathbf{x}) = \\hat{p}(\\mathbf{x}; \\boldsymbol{\\beta}) = \\sigma(\\mathbf{x}^\\top \\boldsymbol{\\beta}), \\quad \\text{where} \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "\n",
    "- $ \\mathbf{x} = (1, x_1, \\dots, x_k)^\\top $ includes a leading 1 for the intercept  \n",
    "- $ \\boldsymbol{\\beta} =$beta_0, \\beta_1, \\dots, \\beta_k)^\\top $ are the parameters we must learn \n",
    "- $ \\sigma(\\cdot) $ maps any real number to the interval (0, 1)\n",
    "\n",
    "Taking logs of the odds yields the familiar linear log-odds relationship:\n",
    "\n",
    "$$\n",
    "\\log\\left[\\frac{\\hat{p}}{1 - \\hat{p}}\\right] = \\mathbf{x}^\\top \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Likelihood and Loss\n",
    "\n",
    "Given $ m $ independent observations $ \\{(\\mathbf{x}^{(i)}, y^{(i)})\\}_{i=1}^{m} $, the Bernoulli likelihood is:\n",
    "\n",
    "$$\n",
    "L$boldsymbol{\\beta}) = \\prod_{i=1}^{m} \\hat{p}^{y^{(i)}} (1 - \\hat{p})^{1 - y^{(i)}}\n",
    "$$\n",
    "\n",
    "Maximising the likelihood is equivalent to minimising the **negative log-likelihood** (also called **cross-entropy loss**):\n",
    "\n",
    "$$\n",
    "J$boldsymbol{\\beta}) = - \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\hat{p}^{(i)} + (1 - y^{(i)}) \\log(1 - \\hat{p}^{(i)}) \\right]\n",
    "$$\n",
    "\n",
    "where $ \\hat{p}^{(i)} = \\sigma(\\mathbf{x}^{(i)\\top} \\boldsymbol{\\beta}) $\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Gradient of the Loss\n",
    "\n",
    "To minimise $ J $, we need its gradient. Using $ \\frac{\\partial \\sigma}{\\partial z} = \\sigma(1 - \\sigma) $:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\beta}} J = \\sum_{i=1}^{m} $hat{p}^{(i)} - y^{(i)}) \\mathbf{x}^{(i)}\n",
    "$$\n",
    "\n",
    "This vector points in the direction of steepest **increase** of the loss; moving opposite to it **reduces** the loss.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Gradient-Descent Update Rule\n",
    "\n",
    "With learning rate $ \\eta > 0 $:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\eta \\nabla_{\\boldsymbol{\\beta}} J$boldsymbol{\\beta}^{(t)})\n",
    "$$\n",
    "\n",
    "Repeat this step until convergence (e.g., loss stabilises or gradient norm is very small).\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Why Not Use a Closed-Form Solution?\n",
    "\n",
    "- **No analytic normal equations**: Unlike OLS, the logistic loss is **non-quadratic**, and setting $ \\nabla_{\\boldsymbol{\\beta}} J = 0 $ yields **non-linear** equations.\n",
    "- **Iterative methods are unavoidable**: Practical solvers rely on:\n",
    "  - First-order: Gradient descent, stochastic gradient descent (SGD)\n",
    "  - Second-order: Newton’s method, L-BFGS\n",
    "\n",
    "**Gradient descent scales better**:\n",
    "- Memory: $ O(k) $\n",
    "- Per-epoch cost: $ O(mk) $\n",
    "- Closed-form would require inverting a $ (k+1) \\times (k+1) $ matrix (cost: $ O(k^3) $), which is infeasible for high-dimensional problems.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Summary of the “From-Scratch” Procedure\n",
    "\n",
    "1. Initialise $ \\boldsymbol{\\beta} $ (e.g., zeros)\n",
    "2. Repeat:\n",
    "   - Compute predictions $ \\hat{p}^{(i)} $ via the sigmoid\n",
    "   - Evaluate the gradient $ \\nabla_{\\boldsymbol{\\beta}} J $\n",
    "   - Update parameters using learning rate\n",
    "3. Terminate when convergence criteria are met\n",
    "\n",
    "This manual gradient-descent routine reproduces the essential mechanics hidden inside high-level libraries, giving you full control over:\n",
    "- Learning rates  \n",
    "- Stopping conditions  \n",
    "- Regularisation  \n",
    "\n",
    "It also deepens your intuition about how logistic regression learns from data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309dc2c-f057-4c96-9489-44f31eacb423",
   "metadata": {},
   "source": [
    "### Why Logistic Regression Has No Closed-Form Solution\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. What “Linear in the Log-Odds” Really Means\n",
    "\n",
    "Logistic regression postulates:\n",
    "\n",
    "$$\n",
    "\\Pr(y = 1 \\mid \\mathbf{x}) = \\sigma(z), \\quad z = \\mathbf{x}^\\top \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\log\\left[\\frac{p}{1 - p}\\right] = \\mathbf{x}^\\top \\boldsymbol{\\beta}\n",
    "$$\n",
    "\n",
    "The right-hand side is **linear in the parameters**, but you never observe $ \\log\\left[\\frac{p}{1 - p}\\right] $.  \n",
    "You only observe $ y \\in \\{0, 1\\} $.\n",
    "\n",
    "In contrast, in ordinary least squares (OLS), you do observe the actual response $ y $ on the **same scale** that the model is linear in, so you can solve:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^\\top \\mathbf{y} \\quad \\Rightarrow \\quad \\hat{\\boldsymbol{\\beta}} = $mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n",
    "$$\n",
    "\n",
    "That direct inversion is what we call the **normal equations**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Why You Cannot “Just Take the Log-Odds of $ y $”\n",
    "\n",
    "- $ y = 0 \\Rightarrow \\log\\left[\\frac{0}{1}\\right] = -\\infty $\n",
    "- $ y = 1 \\Rightarrow \\log\\left[\\frac{1}{0}\\right] = +\\infty $\n",
    "\n",
    "The **logit is undefined** for the binary outcomes themselves, so you cannot transform $ y $ and run OLS.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. The Likelihood Equations Are Nonlinear in $ \\boldsymbol{\\beta} $\n",
    "\n",
    "Maximising the Bernoulli log-likelihood amounts to solving:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{m} (y_i - p_i) \\mathbf{x}_i = 0, \\quad \\text{where} \\quad p_i = \\sigma$mathbf{x}_i^\\top \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "Each $ p_i $ contains the **sigmoid** of a linear form in $ \\boldsymbol{\\beta} $, making the system **nonlinear** in $ \\boldsymbol{\\beta} $.\n",
    "\n",
    "There is **no algebraic trick** to isolate $ \\boldsymbol{\\beta} $ the way $$mathbf{X}^\\top \\mathbf{X})^{-1} $ does for OLS.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Why Gradient-Based (or Newton-Type) Iteration Is Required\n",
    "\n",
    "Because the score equations are **nonlinear**, we need an **iterative root-finding method**, such as:\n",
    "\n",
    "- **First-order methods**: batch or stochastic gradient descent  \n",
    "- **Second-order methods**: Newton–Raphson, IRLS, L-BFGS, etc.\n",
    "\n",
    "These algorithms **repeatedly update $ \\boldsymbol{\\beta} $** until the equations are approximately satisfied.\n",
    "\n",
    "That is why every statistics library **calls an optimiser under the hood** instead of returning a one-shot matrix inverse.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Analogy with Generalised Linear Models (GLMs)\n",
    "\n",
    "Logistic regression is the **canonical GLM** for a Bernoulli response.\n",
    "\n",
    "All GLMs link $ \\mathbb{E}[y] $ to $ \\mathbf{x}^\\top \\boldsymbol{\\beta} $ through a **link function** (logit, log, probit, etc.).\n",
    "\n",
    "Except for the **Gaussian-identity** combination (which gives OLS), none of these likelihoods are quadratic — so **none admit normal equations**.\n",
    "\n",
    "---\n",
    "\n",
    "### Bottom Line\n",
    "\n",
    "The “linear log-odds” phrase refers to the **model structure**, not to a **transform you can directly apply** to the observed $ y $.\n",
    "\n",
    "Because the **log-likelihood remains a nonlinear function of $ \\boldsymbol{\\beta} $**, you must resort to **iterative optimisation** — gradient descent, Newton, or a related method — to obtain the **maximum-likelihood estimates**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b992730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # Sigmoid activation function to squash input to (0, 1)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the logistic regression cost function.\n",
    "    Inputs:\n",
    "        X : Feature matrix of shape (m, n)\n",
    "        y : Target vector of shape (m,)\n",
    "        w : Weights vector of shape (n,)\n",
    "        b : Bias term (scalar)\n",
    "    Returns:\n",
    "        cost : Binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    z = X @ w + b              # Linear combination\n",
    "    h = sigmoid(z)             # Predicted probabilities\n",
    "    # Compute binary cross-entropy loss, with small epsilon for numerical stability\n",
    "    cost = -(1/m) * np.sum(y*np.log(h + 1e-15) + (1 - y)*np.log(1 - h + 1e-15))\n",
    "    return cost\n",
    "\n",
    "def compute_gradients(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the cost function w.r.t. weights and bias.\n",
    "    Inputs:\n",
    "        X, y, w, b : same as above\n",
    "    Returns:\n",
    "        dw : Gradient of cost w.r.t. weights (shape: (n,))\n",
    "        db : Gradient of cost w.r.t. bias (scalar)\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    z = X @ w + b\n",
    "    h = sigmoid(z)\n",
    "    dw = (1/m) * (X.T @ (h - y))       # Gradient w.r.t. weights\n",
    "    db = (1/m) * np.sum(h - y)         # Gradient w.r.t. bias\n",
    "    return dw, db\n",
    "\n",
    "def fit_logistic_regression(X, y, lr=0.01, n_iter=2000, verbose=False):\n",
    "    \"\"\"\n",
    "    Fit a logistic regression model using batch gradient descent.\n",
    "    Inputs:\n",
    "        X : Feature matrix (m, n)\n",
    "        y : Target vector (m,)\n",
    "        lr : Learning rate\n",
    "        n_iter : Number of gradient descent iterations\n",
    "        verbose : If True, print cost every 100 iterations\n",
    "    Returns:\n",
    "        w : Learned weights (n,)\n",
    "        b : Learned bias (scalar)\n",
    "        costs : List of cost values recorded every 100 steps\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)    # Initialize weights to zeros\n",
    "    b = 0.0            # Initialize bias to zero\n",
    "    costs = []\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        # Compute gradients\n",
    "        dw, db = compute_gradients(X, y, w, b)\n",
    "\n",
    "        # Update weights and bias\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "\n",
    "        # Optionally record and print cost\n",
    "        if i % 100 == 0:\n",
    "            cost = compute_cost(X, y, w, b)\n",
    "            costs.append(cost)\n",
    "            if verbose:\n",
    "                print(f'Iter {i}: cost={cost:.4f}')\n",
    "\n",
    "    return w, b, costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1615436f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnQAAAHWCAYAAAD+VRS3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPQtJREFUeJzt3Xt8FPW9x//37CW7ISQLCASQAFGUW0BK8BKs4o20IlZO25+0IuBRW+gRKkUfFqS/gmiLWqvYFlCPR6meKrQHa/21SI1yVVSQi6Ig2iIEIYDcknDJJtn9/v5IdpMlAUOys5MNr+fjsY/sfuc7M9/deahvPzPzHcsYYwQAAICk5XJ6AAAAAGgaAh0AAECSI9ABAAAkOQIdAABAkiPQAQAAJDkCHQAAQJIj0AEAACQ5Ah0AAECSI9ABAAAkOQIdAEdYltWg14oVK5q0n5kzZ8qyrEatu2LFiriMIdn2DSD5eJweAICz07vvvhvz+cEHH9Ty5cu1bNmymPa+ffs2aT933nmnvv3tbzdq3UGDBundd99t8hgAwG4EOgCOuOyyy2I+d+jQQS6Xq077yY4fP65WrVo1eD9du3ZV165dGzXGjIyMrx0PADQHnHIF0GxdddVVysnJ0apVqzRkyBC1atVKt99+uyRp0aJFys/PV+fOnZWamqo+ffpo6tSpOnbsWMw26jvl2qNHD40YMUJLly7VoEGDlJqaqt69e+u5556L6Vffac/bbrtNrVu31r/+9S8NHz5crVu3VlZWlu655x4Fg8GY9b/88kt9//vfV3p6utq0aaPRo0dr3bp1sixLCxYsaNRv8tprrykvL0+tWrVSenq6hg0bVqfa+dVXX+nHP/6xsrKy5PP51KFDB11++eV68803o302btyoESNGqGPHjvL5fOrSpYtuuOEGffnll40aFwBnUaED0KwVFRXp1ltv1X333adf//rXcrmq/j/0888/1/DhwzV58mSlpaXp008/1SOPPKK1a9fWOW1bnw8//FD33HOPpk6dqszMTD377LO644471LNnT1155ZWnXbeiokLf+c53dMcdd+iee+7RqlWr9OCDDyoQCOiXv/ylJOnYsWO6+uqrdejQIT3yyCPq2bOnli5dqlGjRjX6t3jppZc0evRo5efn6+WXX1YwGNSjjz6qq666Sm+99Za++c1vSpLGjBmjDRs26Fe/+pUuvPBCHTlyRBs2bNDBgwejYxs2bJiys7M1d+5cZWZmau/evVq+fLlKS0sbPT4ADjIA0AyMGzfOpKWlxbQNHTrUSDJvvfXWadcNh8OmoqLCrFy50kgyH374YXTZjBkzzMn/quvevbvx+/1m586d0bYTJ06Ydu3amfHjx0fbli9fbiSZ5cuXx4xTkvnzn/8cs83hw4ebXr16RT/PnTvXSDKvv/56TL/x48cbSeb5558/7Xc6ed+hUMh06dLF9O/f34RCoWi/0tJS07FjRzNkyJBoW+vWrc3kyZNPue0PPvjASDKvvvrqaccAIHlwyhVAs9a2bVtdc801ddq3b9+uW265RZ06dZLb7ZbX69XQoUMlSVu3bv3a7Q4cOFDdunWLfvb7/brwwgu1c+fOr13XsizdeOONMW0DBgyIWXflypVKT0+vc0PGD3/4w6/dfn22bdumPXv2aMyYMdEqpSS1bt1a3/ve9/Tee+/p+PHjkqRLLrlECxYs0EMPPaT33ntPFRUVMdvq2bOn2rZtq5///Od66qmntGXLlkaNCUDzQaAD0Kx17ty5TtvRo0d1xRVX6P3339dDDz2kFStWaN26dXrllVckSSdOnPja7Z5zzjl12nw+X4PWbdWqlfx+f511y8rKop8PHjyozMzMOuvW19YQkdOl9f0eXbp0UTgc1uHDhyVVXV84btw4Pfvss8rLy1O7du00duxY7d27V5IUCAS0cuVKDRw4UPfff7/69eunLl26aMaMGXXCH4DkwDV0AJq1+uaQW7Zsmfbs2aMVK1ZEq3KSdOTIkQSO7PTOOeccrV27tk57JFQ1ZntS1TWFJ9uzZ49cLpfatm0rSWrfvr3mzJmjOXPmqLCwUK+99pqmTp2q/fv3a+nSpZKk/v37a+HChTLG6KOPPtKCBQs0a9YspaamaurUqY0aIwDnUKEDkHQiIc/n88W0P/30004Mp15Dhw5VaWmpXn/99Zj2hQsXNmp7vXr10rnnnquXXnpJxpho+7Fjx7R48eLona8n69atmyZOnKhhw4Zpw4YNdZZblqWLLrpITzzxhNq0aVNvHwDNHxU6AElnyJAhatu2rSZMmKAZM2bI6/XqT3/6kz788EOnhxY1btw4PfHEE7r11lv10EMPqWfPnnr99df1z3/+U5JiroNrCJfLpUcffVSjR4/WiBEjNH78eAWDQf3mN7/RkSNH9PDDD0uSiouLdfXVV+uWW25R7969lZ6ernXr1mnp0qX67ne/K0n6+9//rnnz5mnkyJE677zzZIzRK6+8oiNHjmjYsGHx/SEAJASBDkDSOeecc/SPf/xD99xzj2699ValpaXppptu0qJFizRo0CCnhydJSktL07JlyzR58mTdd999sixL+fn5mjdvnoYPH642bdqc8TZvueUWpaWlafbs2Ro1apTcbrcuu+wyLV++XEOGDJFUdXPHpZdeqhdffFE7duxQRUWFunXrpp///Oe67777JEkXXHCB2rRpo0cffVR79uxRSkqKevXqpQULFmjcuHHx/BkAJIhlatfuAQC2+vWvf61f/OIXKiwsbPQTLADgZFToAMAmf/jDHyRJvXv3VkVFhZYtW6bf/e53uvXWWwlzAOKKQAcANmnVqpWeeOIJ7dixQ8FgMHrq8xe/+IXTQwPQwnDKFQAAIMkxbQkAAECSI9ABAAAkOQIdAABAkjvrbooIh8Pas2eP0tPT632kEAAAgJOMMSotLVWXLl0aPAn5WRfo9uzZo6ysLKeHAQAAcFq7du1q8BRHZ12gS09Pl1T1I2VkZDg8GgAAgFglJSXKysqKZpaGOOsCXeQ0a0ZGBoEOAAA0W2dyaRg3RQAAACQ5Ah0AAECSI9ABAAAkOQIdAABAkiPQAQAAJDkCHQAAQJIj0AEAACQ5Ah0AAECSI9ABAAAkOQIdAABAkiPQAQAAJDkCHQAAQJIj0AEAACQ5j9MDaIlKyip08Gi5Ur1udQr4nR4OAABo4ajQ2eBP7xXq6sdW6LE3tjk9FAAAcBYg0NnA7636WcsqQg6PBAAAnA0IdDbwedySpGBl2OGRAACAswGBzgZU6AAAQCIR6GxAhQ4AACQSgc4GkQpdkAodAABIAAKdDajQAQCARCLQ2YBr6AAAQCIR6GxAhQ4AACQSgc4GVOgAAEAiEehsQIUOAAAkEoHOBr7IXa4EOgAAkAAEOhv4qyt0obBRRYhQBwAA7EWgs0GkQidRpQMAAPYj0NnA56n5WbkxAgAA2M3xQDdv3jxlZ2fL7/crNzdXq1evPmXfFStWyLKsOq9PP/00gSP+epZlKcXDdXQAACAxHA10ixYt0uTJkzV9+nRt3LhRV1xxha6//noVFhaedr1t27apqKgo+rrgggsSNOKG83uYugQAACSGo4Hu8ccf1x133KE777xTffr00Zw5c5SVlaX58+efdr2OHTuqU6dO0Zfb7U7QiBvO562euqSCCh0AALCXY4GuvLxc69evV35+fkx7fn6+1qxZc9p1v/GNb6hz58669tprtXz5cjuH2WjRyYUrqdABAAB7eZza8YEDBxQKhZSZmRnTnpmZqb1799a7TufOnfXMM88oNzdXwWBQL774oq699lqtWLFCV155Zb3rBINBBYPB6OeSkpL4fYnTiE4uTIUOAADYzLFAF2FZVsxnY0ydtohevXqpV69e0c95eXnatWuXHnvssVMGutmzZ+uBBx6I34AbiAodAABIFMdOubZv315ut7tONW7//v11qnanc9lll+nzzz8/5fJp06apuLg4+tq1a1ejx3wmqNABAIBEcSzQpaSkKDc3VwUFBTHtBQUFGjJkSIO3s3HjRnXu3PmUy30+nzIyMmJeieCLTltChQ4AANjL0VOuU6ZM0ZgxYzR48GDl5eXpmWeeUWFhoSZMmCCpqrq2e/duvfDCC5KkOXPmqEePHurXr5/Ky8v1v//7v1q8eLEWL17s5Neol5+7XAEAQII4GuhGjRqlgwcPatasWSoqKlJOTo6WLFmi7t27S5KKiopi5qQrLy/Xvffeq927dys1NVX9+vXTP/7xDw0fPtypr3BKVOgAAECiWMYY4/QgEqmkpESBQEDFxcW2nn792aJN+uvG3Zo+vI9+dOV5tu0HAAC0LI3JKo4/+qulokIHAAAShUBnk8g1dGVcQwcAAGxGoLMJFToAAJAoBDqb+KjQAQCABCHQ2YQKHQAASBQCnU0igY4KHQAAsBuBzibRiYWp0AEAAJsR6GxChQ4AACQKgc4mVOgAAECiEOhsUnNTBBU6AABgLwKdTZhYGAAAJAqBziZMWwIAABKFQGeT6DV0VOgAAIDNCHQ28Xmp0AEAgMQg0NnE7+EaOgAAkBgEOptQoQMAAIlCoLOJr7pCVxEyCoWNw6MBAAAtGYHOJn5vzU9LlQ4AANiJQGeTSIVO4jo6AABgLwKdTdwuS163JYkKHQAAsBeBzkaRKh1z0QEAADsR6GwUuY6ujAodAACwEYHORlToAABAIhDobBSZi66sggodAACwD4HORtEKXSUVOgAAYB8CnY38VOgAAEACEOhs5PNEHv9FhQ4AANiHQGejyClXKnQAAMBOBDobRU65UqEDAAB2ItDZiAodAABIBAKdjajQAQCARCDQ2YhpSwAAQCIQ6GwUrdBxyhUAANiIQGcjKnQAACARCHQ2YmJhAACQCAQ6G1GhAwAAiUCgsxEVOgAAkAgEOhtRoQMAAIlAoLORjwodAABIAAKdjajQAQCARCDQ2YgKHQAASAQCnY38VOgAAEACEOhs5Is+y5UKHQAAsA+BzkaRCl1ZBRU6AABgHwKdjXw8yxUAACQAgc5Gfm91hY5r6AAAgI0IdDbyeap+3vLKsIwxDo8GAAC0VAQ6G0UqdBJ3ugIAAPsQ6GwUqdBJUpAbIwAAgE0IdDbyuCy5rKr3ZUxdAgAAbEKgs5FlWdHTrlToAACAXQh0NoucdqVCBwAA7EKgsxkVOgAAYDcCnc2o0AEAALsR6GxGhQ4AANiNQGezSIUuSIUOAADYhEBnM1/k8V9U6AAAgE0IdDajQgcAAOxGoLOZnwodAACwGYHOZlToAACA3Qh0NvN5qNABAAB7Eehs5vdSoQMAAPYi0NmMCh0AALCb44Fu3rx5ys7Olt/vV25urlavXt2g9d555x15PB4NHDjQ3gE2ERU6AABgN0cD3aJFizR58mRNnz5dGzdu1BVXXKHrr79ehYWFp12vuLhYY8eO1bXXXpugkTYeFToAAGA3RwPd448/rjvuuEN33nmn+vTpozlz5igrK0vz588/7Xrjx4/XLbfcory8vASNtPGo0AEAALs5FujKy8u1fv165efnx7Tn5+drzZo1p1zv+eef17///W/NmDHD7iHGRc20JVToAACAPTxO7fjAgQMKhULKzMyMac/MzNTevXvrXefzzz/X1KlTtXr1ank8DRt6MBhUMBiMfi4pKWn8oBshMrFwsIIKHQAAsIfjN0VYlhXz2RhTp02SQqGQbrnlFj3wwAO68MILG7z92bNnKxAIRF9ZWVlNHvOZ8Hmp0AEAAHs5Fujat28vt9tdpxq3f//+OlU7SSotLdUHH3ygiRMnyuPxyOPxaNasWfrwww/l8Xi0bNmyevczbdo0FRcXR1+7du2y5fucij96UwQVOgAAYA/HTrmmpKQoNzdXBQUF+o//+I9oe0FBgW666aY6/TMyMrR58+aYtnnz5mnZsmX6v//7P2VnZ9e7H5/PJ5/PF9/BnwEqdAAAwG6OBTpJmjJlisaMGaPBgwcrLy9PzzzzjAoLCzVhwgRJVdW13bt364UXXpDL5VJOTk7M+h07dpTf76/T3pz4qNABAACbORroRo0apYMHD2rWrFkqKipSTk6OlixZou7du0uSioqKvnZOuubOT4UOAADYzDLGGKcHkUglJSUKBAIqLi5WRkaG7fv7eHexRvz+bWVm+PT+/dfZvj8AAJDcGpNVHL/LtaWjQgcAAOxGoLMZ19ABAAC7EehsVvsu17Ps7DYAAEgQAp3NIhU6Y6SKEIEOAADEH4HOZpFr6CSprJLTrgAAIP4IdDZLcbsUeZJZsIIbIwAAQPwR6GxmWZZ8nqqfmRsjAACAHQh0CRC5jo6pSwAAgB0IdAlAhQ4AANiJQJcAfi8VOgAAYB8CXQJEKnRBKnQAAMAGBLoEoEIHAADsRKBLAK6hAwAAdiLQJQAVOgAAYCcCXQJEr6HjSREAAMAGBLoEiFToynhSBAAAsAGBLgGo0AEAADsR6BLAR4UOAADYiECXAFToAACAnQh0CeDzRqYtoUIHAADij0CXAH5PZNoSKnQAACD+CHQJQIUOAADYiUCXADUVOgIdAACIPwJdAtRU6DjlCgAA4o9AlwBU6AAAgJ0IdAkQqdAFqdABAAAbEOgSIFKhK6NCBwAAbECgSwAqdAAAwE4EugTwe7mGDgAA2IdAlwDRR39RoQMAADYg0CWAj2voAACAjQh0CeDnGjoAAGAjAl0CUKEDAAB2ItAlQKRCFwobVYYIdQAAIL4IdAkQqdBJVOkAAED8EegSIHKXq8R1dAAAIP4IdAngcllKcVf91FToAABAvBHoEoSnRQAAALsQ6BIkch0dT4sAAADxRqBLkMidrmVU6AAAQJwR6BIk+vgvKnQAACDOCHQJEp1cmAodAACIMwJdgkQf/0WFDgAAxBmBLkGo0AEAALsQ6BKECh0AALALgS5BotOWUKEDAABxRqBLECp0AADALgS6BOEaOgAAYBcCXYJQoQMAAHYh0CWIz8ujvwAAgD0IdAni9/DoLwAAYA8CXYJEK3QVVOgAAEB8EegSJPIs17JKKnQAACC+CHQJQoUOAADYhUCXIFToAACAXQh0CeKnQgcAAGxCoEsQKnQAAMAuBLoEoUIHAADsQqBLECp0AADALgS6BKFCBwAA7EKgS5BIhY5HfwEAgHgj0CVITYWOU64AACC+CHQJQoUOAADYpVGBbtasWTp+/Hid9hMnTmjWrFlNHlRLFAl05aGwQmHj8GgAAEBL0qhA98ADD+jo0aN12o8fP64HHnjgjLY1b948ZWdny+/3Kzc3V6tXrz5l37fffluXX365zjnnHKWmpqp379564oknznj8ToiccpWkcqp0AAAgjjyNWckYI8uy6rR/+OGHateuXYO3s2jRIk2ePFnz5s3T5ZdfrqefflrXX3+9tmzZom7dutXpn5aWpokTJ2rAgAFKS0vT22+/rfHjxystLU0//vGPG/NVEiZSoZOksoqQUlPcp+kNAADQcJYxpsHn/9q2bSvLslRcXKyMjIyYUBcKhXT06FFNmDBBc+fObdD2Lr30Ug0aNEjz58+PtvXp00cjR47U7NmzG7SN7373u0pLS9OLL77YoP4lJSUKBALR75BIPe9fosqw0XvTrlWngD+h+wYAAMmhMVnljCp0c+bMkTFGt99+ux544AEFAoHospSUFPXo0UN5eXkN2lZ5ebnWr1+vqVOnxrTn5+drzZo1DdrGxo0btWbNGj300EOn7BMMBhUMBqOfS0pKGrRtO/g8LlWWh1TGna4AACCOzijQjRs3TpKUnZ2tyy+/XB5Po87YSpIOHDigUCikzMzMmPbMzEzt3bv3tOt27dpVX331lSorKzVz5kzdeeedp+w7e/bsM76uzy5+r1vHykPc6QoAAOKqUTdFpKena+vWrdHPf/vb3zRy5Ejdf//9Ki8vP6NtnXwt3qmuz6tt9erV+uCDD/TUU09pzpw5evnll0/Zd9q0aSouLo6+du3adUbji6fo47+o0AEAgDhqVKAbP368PvvsM0nS9u3bNWrUKLVq1Up/+ctfdN999zVoG+3bt5fb7a5Tjdu/f3+dqt3JsrOz1b9/f/3oRz/Sz372M82cOfOUfX0+nzIyMmJeTolOLkyFDgAAxFGjAt1nn32mgQMHSpL+8pe/aOjQoXrppZe0YMECLV68uEHbSElJUW5urgoKCmLaCwoKNGTIkAaPxRgTc41cc5YSnVyYCh0AAIifRk9bEg5XVZnefPNNjRgxQpKUlZWlAwcONHg7U6ZM0ZgxYzR48GDl5eXpmWeeUWFhoSZMmCCp6nTp7t279cILL0iS5s6dq27duql3796Squale+yxxzRp0qTGfI2E81VX6MoqqNABAID4aVSgGzx4sB566CFdd911WrlyZXTakS+++OJrT5fWNmrUKB08eFCzZs1SUVGRcnJytGTJEnXv3l2SVFRUpMLCwmj/cDisadOm6YsvvpDH49H555+vhx9+WOPHj2/M10g4PxU6AABggzOahy7io48+0ujRo1VYWKgpU6ZoxowZkqRJkybp4MGDeumll+I+0Hhxch66sc+t1arPvtJj/89F+n5u14TuGwAAJAfb56GLGDBggDZv3lyn/Te/+Y3cbp6AcCpU6AAAgB0aP5GcpPXr12vr1q2yLEt9+vTRoEGD4jWuFolr6AAAgB0aFej279+vUaNGaeXKlWrTpo2MMSouLtbVV1+thQsXqkOHDvEeZ4tAhQ4AANihUdOWTJo0SaWlpfrkk0906NAhHT58WB9//LFKSkr005/+NN5jbDF83sjEwlToAABA/DSqQrd06VK9+eab6tOnT7Stb9++mjt3rvLz8+M2uJbG74lMLEyFDgAAxE+jKnThcFher7dOu9frjc5Ph7oiFbogFToAABBHjQp011xzje6++27t2bMn2rZ792797Gc/07XXXhu3wbU0VOgAAIAdGhXo/vCHP6i0tFQ9evTQ+eefr549eyo7O1ulpaX6/e9/H+8xthhcQwcAAOzQqGvosrKytGHDBhUUFOjTTz+VMUZ9+/bVddddF+/xtSg+KnQAAMAGZ1ShW7Zsmfr27auSkhJJ0rBhwzRp0iT99Kc/1cUXX6x+/fpp9erVtgy0JfBzDR0AALDBGQW6OXPm6Ec/+lG9j6EIBAIaP368Hn/88bgNrqWJVOjKqNABAIA4OqNA9+GHH+rb3/72KZfn5+dr/fr1TR5US0WFDgAA2OGMAt2+ffvqna4kwuPx6KuvvmryoFoqKnQAAMAOZxTozj33XG3evPmUyz/66CN17ty5yYNqqZiHDgAA2OGMAt3w4cP1y1/+UmVlZXWWnThxQjNmzNCIESPiNriWhgodAACwwxlNW/KLX/xCr7zyii688EJNnDhRvXr1kmVZ2rp1q+bOnatQKKTp06fbNdakxzV0AADADmcU6DIzM7VmzRr95Cc/0bRp02SMkSRZlqVvfetbmjdvnjIzM20ZaEsQrdBVUKEDAADxc8YTC3fv3l1LlizR4cOH9a9//UvGGF1wwQVq27atHeNrUaIVukoqdAAAIH4a9aQISWrbtq0uvvjieI6lxat5UkRYxhhZluXwiAAAQEvQqGe5onEid7lKVOkAAED8EOgSyF9doZMIdAAAIH4IdAnkdVuKnGUNcmMEAACIEwJdAlmWFa3SUaEDAADxQqBLsMh1dExdAgAA4oVAl2BU6AAAQLwR6BKMCh0AAIg3Al2CUaEDAADxRqBLMCp0AAAg3gh0CUaFDgAAxBuBLsGo0AEAgHgj0CWYz1P1k1OhAwAA8UKgSzCft/qUKxU6AAAQJwS6BItU6Mqo0AEAgDgh0CWYP1qhI9ABAID4INAlWE2FjlOuAAAgPgh0CUaFDgAAxBuBLsGo0AEAgHgj0CUYFToAABBvBLoEo0IHAADijUCXYFToAABAvBHoEqzmSRFU6AAAQHwQ6BLM56FCBwAA4otAl2B+LxU6AAAQXwS6BItU6Mqo0AEAgDgh0CUYFToAABBvBLoEo0IHAADijUCXYFToAABAvBHoEowKHQAAiDcCXYLVrtAZYxweDQAAaAkIdAkWqdCFjVQRItABAICmI9AlmM9b85NzHR0AAIgHAl2CRR79JXEdHQAAiA8CXYJZlqUUnucKAADiiEDnAH800FGhAwAATUegc4DPG5m6hAodAABoOgKdA2qmLqFCBwAAmo5A54CayYWp0AEAgKYj0DmACh0AAIgnAp0DIhW6IBU6AAAQBwQ6B1ChAwAA8USgcwDX0AEAgHgi0DmACh0AAIgnAp0DqNABAIB4cjzQzZs3T9nZ2fL7/crNzdXq1atP2feVV17RsGHD1KFDB2VkZCgvL0///Oc/Ezja+Ig8zzXIs1wBAEAcOBroFi1apMmTJ2v69OnauHGjrrjiCl1//fUqLCyst/+qVas0bNgwLVmyROvXr9fVV1+tG2+8URs3bkzwyJvGH3lSBM9yBQAAcWAZY4xTO7/00ks1aNAgzZ8/P9rWp08fjRw5UrNnz27QNvr166dRo0bpl7/8ZYP6l5SUKBAIqLi4WBkZGY0ad1PNXrJVT6/arju/ma1fjOjryBgAAEDz1Jis4liFrry8XOvXr1d+fn5Me35+vtasWdOgbYTDYZWWlqpdu3an7BMMBlVSUhLzclrkWa7cFAEAAOLBsUB34MABhUIhZWZmxrRnZmZq7969DdrGb3/7Wx07dkw333zzKfvMnj1bgUAg+srKymrSuOMhcg0dN0UAAIB4cPymCMuyYj4bY+q01efll1/WzJkztWjRInXs2PGU/aZNm6bi4uLoa9euXU0ec1P5qdABAIA48ji14/bt28vtdtepxu3fv79O1e5kixYt0h133KG//OUvuu66607b1+fzyefzNXm88USFDgAAxJNjFbqUlBTl5uaqoKAgpr2goEBDhgw55Xovv/yybrvtNr300ku64YYb7B6mLajQAQCAeHKsQidJU6ZM0ZgxYzR48GDl5eXpmWeeUWFhoSZMmCCp6nTp7t279cILL0iqCnNjx47Vk08+qcsuuyxa3UtNTVUgEHDse5wpKnQAACCeHA10o0aN0sGDBzVr1iwVFRUpJydHS5YsUffu3SVJRUVFMXPSPf3006qsrNRdd92lu+66K9o+btw4LViwINHDbzQqdAAAIJ4cnYfOCc1hHrpVn32lsc+tVe9O6Vo6+UpHxgAAAJqnpJqH7mwWOeVaToUOAADEAYHOAdFHf3ENHQAAiAMCnQN83qqfnWvoAABAPBDoHOD3cFMEAACIHwKdAyIVOk65AgCAeCDQOSBSoasMG1WGqNIBAICmIdA5IFKhkzjtCgAAmo5A5wBfdYVOItABAICmI9A5wO2y5HVbkriODgAANB2BziHc6QoAAOKFQOcQ7nQFAADxQqBziI8KHQAAiBMCnUOo0AEAgHgh0DmECh0AAIgXAp1D/JHnuVKhAwAATUSgc4jPU33KlQodAABoIgKdQ/ze6lOuVOgAAEATEegcQoUOAADEC4HOIVToAABAvBDoHBKp0HGXKwAAaCoCnUOo0AEAgHgh0DmEa+gAAEC8EOgcEp1YmAodAABoIgKdQ/zRR39RoQMAAE1DoHNIzaO/qNABAICmIdA5JProL66hAwAATUSgc0ikQlfGNXQAAKCJCHQO8VGhAwAAcUKgcwgVOgAAEC8EOodwDR0AAIgXAp1DqNABAIB4IdA5hAodAACIFwKdQ2oqdAQ6AADQNAQ6h9Tc5copVwAA0DQEOof4vZFnuVKhAwAATUOgc4jPU/XTl4fCCoeNw6MBAADJjEDnkEiFTqoKdQAAAI1FoHNIpEInMXUJAABoGgKdQ7xul9wuSxJTlwAAgKYh0DkoUqWjQgcAAJqCQOeg6J2uVOgAAEATEOgcRIUOAADEA4HOQVToAABAPBDoHESFDgAAxAOBzkGRQMfTIgAAQFMQ6Bzkqz7lWsbzXAEAQBMQ6BxEhQ4AAMQDgc5Bfip0AAAgDgh0DqJCBwAA4oFA5yCmLQEAAPFAoHMQ05YAAIB4INA5iAodAACIBwKdg6jQAQCAeCDQOYgKHQAAiAcCnYNq7nKlQgcAABqPQOegaKCjQgcAAJqAQOeg6MTCVOgAAEATEOgc5PNSoQMAAE1HoHOQ30OFDgAANB2BzkFU6AAAQDwQ6BwUqdAFK6nQAQCAxiPQOShSoSuroEIHAAAaj0DnIF91he54eaWMMQ6PBgAAJCvHA928efOUnZ0tv9+v3NxcrV69+pR9i4qKdMstt6hXr15yuVyaPHly4gZqg65tU5XicenA0XItWLPD6eEAAIAk5WigW7RokSZPnqzp06dr48aNuuKKK3T99dersLCw3v7BYFAdOnTQ9OnTddFFFyV4tPHXplWKpg/vI0maveRTfby72OERAQCAZGQZB8/1XXrppRo0aJDmz58fbevTp49Gjhyp2bNnn3bdq666SgMHDtScOXPOaJ8lJSUKBAIqLi5WRkZGY4YdV8YY/fjF9SrYsk/ntU/T/zfpm0rzeZweFgAAcEhjsopjFbry8nKtX79e+fn5Me35+flas2ZN3PYTDAZVUlIS82pOLMvSo98boM4Bv7YfOKYZr33i9JAAAECScSzQHThwQKFQSJmZmTHtmZmZ2rt3b9z2M3v2bAUCgegrKysrbtuOl7ZpKZozaqBclvR/67/U3zbtdnpIAAAgiTh+U4RlWTGfjTF12ppi2rRpKi4ujr527doVt23H06XnnaNJ11wgSZr+14+18+Axh0cEAACShWOBrn379nK73XWqcfv3769TtWsKn8+njIyMmFdzNemanrqkRzsdDVZq0ssbVc4TJAAAQAM4FuhSUlKUm5urgoKCmPaCggINGTLEoVE5y+N2ac4PBiqQ6tVHXxbrsTe2OT0kAACQBBw95TplyhQ9++yzeu6557R161b97Gc/U2FhoSZMmCCp6nTp2LFjY9bZtGmTNm3apKNHj+qrr77Spk2btGXLFieGb4subVL1m+8PkCQ9s2q7Vmzb7/CIAABAc+fo/BijRo3SwYMHNWvWLBUVFSknJ0dLlixR9+7dJVVNJHzynHTf+MY3ou/Xr1+vl156Sd27d9eOHTsSOXRb5ffrpLF53fXCuzt1718+1JK7r1DHdL/TwwIAAM2Uo/PQOaG5zUN3KmUVIY2c+44+3Vuqb/Zsrxduv0QuV/xuFgEAAM1TUs1Dh9Pze936wy3fUKrXrbf/dUBPr9ru9JAAAEAzRaBrxnp2TNfM7/SVJP32jW3aWHjY4REBAIDmiEDXzN08OEsjBnRWZdho4ksb9eGuI04PCQAANDMEumbOsiz9+rv91a1dK+0+ckIj572jaa98pEPHyp0eGgAAaCYIdEkgw+/V//0kT//xjXNljPTy2l265rcr9L/v7VQofFbd0wIAAOrBXa5JZu0Xh/TLv32sT/eWSpJyzs3QrJtyNKhbW4dHBgAA4qExWYVAl4QqQ2G9+N5OPf7GZyoNVkqSbh7cVT//dm+d09rn8OgAAEBTMG3JWcLjduk/L8/Wsnuv0vcGdZUk/fmDL3X1Yyv0wrs7OA0LAMBZhgpdC7B+5yH9v69+oi1FJZKkPp0zdPvlPfStnE7K8HsdHh0AADgTnHJtgJYY6CQpFDb60/s79dg/t6mkrOo0bIrHpev6dNR3LjpXV/XqIL/X7fAoAQDA1yHQNUBLDXQRB48G9af3C/W3Tbv176+ORdvT/R5dn9NJNw08V5edd47cPEYMAIBmiUDXAC090EUYY/TJnhK99uEevbZpj/aWlEWXdUz36caLuuimgV3U/9yALItwBwBAc0Gga4CzJdDVFg4brd1xSH/btEdLNhep+ERFdFn71j5dkt1WF/dop4t7tFOfzhlU7wAAcBCBrgHOxkBXW7AypFWfHdDfNu3Wm1v3qawiHLM83efRoO5tdUl2VcAb0DXAtXcAACQQga4BzvZAV1tZRUgffVmsdTsO6f0vDmnDzsM6Wj2vXUSKx6WLugY0qFtb9eqUrgsz09WzY2tCHgAANiHQNQCB7tQqQ2F9urdUa784pHU7ql4HjtZ9ZqzbZanHOa3Uu1NGNOT17pSubu1aycXpWgAAmoRA1wAEuoYzxuiLA8e0bschfby7RNv2lWrb3tKYa/BqS/W61bNja3U/p5W6tWtV/TdN3c5ppc4ZfsIeAAANQKBrAAJd0xhjtK8kqE/3lmjb3qqAt21fqT7ff1TlleFTrpfidqlru9SqoNeulbLatdK5bVLVKeBX50CqOqT7uBkDAAAR6BqEQGePylBYOw4e17/2H9WuQ8e189Ax7Tx4XLsOHdeXh0+o8mseR+Z2WeqY7lNmhl+dA/7qoOdXp0CqOqb71L61T+1bpyiQ6mWaFQBAi9aYrOKxeUw4S3jcLvXs2Fo9O7aus6wyFFZRcZkKDx3XzoPHVXioKugVFZ/Q3uIy7SsNKhQ2KiouU1FxmTbtOvV+vG5L56T51D49pTrk1YS99q19atPKq7atUtQuLUVtWnnV2uchAAIAWjwCHWzncbuUVX2a9fKedZeHwkYHjwajgW5v8QntLQlqb/EJFRWX6aujQR0oDaqkrFIVIaO9JWUxEyWfjtdtKZCaonZpXrVplaK21YEvkOpVRvUrkOpVht8Tbav67FWKxxXnXwIAAHsQ6OA4t8tSxwy/Omb4dVHWqfsFK0M6eLRcB44Gq16l5VVh72hQB46W69CxoA4fq9CR4+U6dLxcZRVhVYRMtP+ZSvW6le73qLXfo3S/V+k+T9VnX922NF9Ve6sUd817n1utfR6let1UCQEAtiLQIWn4PG51aZOqLm1SG9S/rCKkw8fLdehYuY4cr9Dh4+U6fKxch49XqOREhYpPVKikrOpv8YlKlZyoai+tnovvREVIJypC2l965mGwNsuS0lI8SvO5lZbiUWpKzd9WKe7o39g2j1p5q5alet3y13qfGnmf4pbf45LHTSURAM52BDq0WH6vW50DqeocaFgAjAiFjUqrg15pWaVKyyp1NFipo8HYz6VlFTpa6/Ox8kodD4aq3gcrdaw8JEkyRtXrV0pqWjisj9dtye9xy+d1y+91yR/563FH3/u87urPLvk8bvm8Lvk8VX19nqq26DKPSykeV62/7uhnX63PKR4XdyYDQDNBoANO4nZZatMqRW1apTRpO+Gw0YmKUDTcHasOescrQjoeDOl4eaVOVIR0vLzqdaK8qt+J8siysMrKQ9FK4YnykMoqaj5H7k+vCBlVhCqjlcVE8rgspXhc8rqrAl6KuyYIRj5H3nvdNZ+97rrred21+1nyVrd5PVWfPa6q9163pRR3VWXSW6tfZCwel1XVz+WSx23J47I45Q2gxSPQATZxuSyl+aqur4s3Y4yCleGqkFcZCXthlVVWhb5gRVhlFaHqz9XvK8Iqr6zqE6wIK1i9LFgZUrAyrGBlVb9gZVW/YGWo+m/N52BlWLUnOqoMG1WWhySF4v4d48lbHQg91QHQ44oEQUueWp89bisaBN2u2L6eSKistcztsqKh0eOq6hvZXqTdXStYul014zjd58h2qz7XtLtcqlle3R55T2gFzm4EOiAJWZZVfTo1sc/UNcaoMmxigl55ZVjloZogWBEy0bZIv0hbRSjSPxz9XNNW1acyHGkz0eUVoarlFbXWqapMhlUZrmovr34fqmfOw6q+Ian+h5y0CC6rJuyd/PK4LLms6pBo1bPspD4uKzYwuquDqduq+h+Vk7fhOumz27Jq9Ytdp/Y+Yvq6VLctsk6kLbJ/V+z6lqWYsbiqP7tq7dPlUnR7Vj3rRtYjGCNZEegANJhlWdHTnGk+p0dTv3DYqCJcHfgqw6oIh1UZMqoMmej7SBCsrA6GlSe1V4SqPofC9a8TCY4VIaNQ9b5C4ZrtRLYRNqbWsth1o59DRiFTs36oev1In2h79f5O+b2NVB4KN/diabPnslQdAGNDZO2gaFn1L4u8ry8onrzMFQ2XqhU8VbNtl2L3Y0WCaM372sE1sp3T9a0dWiP7q2+566SAG7vdSL/ayyO/2an7W4psN7aPpVrbdMWO0VLNviyr1vqqZ9+WJculuuucNM6WHNgJdABaFJfLks/lls8jqZmGzqYIVwe9cHW1NBSqCXwnB8FIGAyHVfXXmGhQDVWvH91ere2G6mmrDNUsC5mqZaGwFAqHqwOpTtOv1ntTvU64Zp1Qrf3GvDeKrh82plZfRfsaY2L2H+0b2WdkG8aoIc9FCpuq7ehrnm6D5HVywKsdEl2WJZ0iCNaERWnStRfoh5d0c/qrxCDQAUAScbkspbhabpXBTsbUhMGTA2JNADQKhxUNoOFa60TCY7g6PMasXytEmlr7MHX2JxlFwqii26+9r7CpCaWRz8YoOj5Te7xhIyPVbNuctJ1w7DZj9lnPWCPfydT6vSL9o9sO17+9U/WPbE/17Dtca7ymVt9w9e8T+W717aOxDy4NV3256mJ24zZyzIGb0L4OgQ4AcFaoOoUppttpQU4OeDF/VRVWVetz7SBaO1DWt66Jhsi663UK+B3+5nUR6AAAQFKKhnQR0pliHgAAIMkR6AAAAJIcgQ4AACDJEegAAACSHIEOAAAgyRHoAAAAkhyBDgAAIMkR6AAAAJIcgQ4AACDJEegAAACSHIEOAAAgyRHoAAAAkhyBDgAAIMkR6AAAAJKcx+kBJJoxRpJUUlLi8EgAAADqimSUSGZpiLMu0JWWlkqSsrKyHB4JAADAqZWWlioQCDSor2XOJP61AOFwWHv27FF6erosy7JtPyUlJcrKytKuXbuUkZFh235wZjguzRfHpvni2DRfHJvmqynHxhij0tJSdenSRS5Xw66OO+sqdC6XS127dk3Y/jIyMviHrBniuDRfHJvmi2PTfHFsmq/GHpuGVuYiuCkCAAAgyRHoAAAAkhyBziY+n08zZsyQz+dzeiiohePSfHFsmi+OTfPFsWm+En1szrqbIgAAAFoaKnQAAABJjkAHAACQ5Ah0AAAASY5AZ4N58+YpOztbfr9fubm5Wr16tdNDalFWrVqlG2+8UV26dJFlWXr11VdjlhtjNHPmTHXp0kWpqam66qqr9Mknn8T0CQaDmjRpktq3b6+0tDR95zvf0ZdffhnT5/DhwxozZowCgYACgYDGjBmjI0eO2Pztktfs2bN18cUXKz09XR07dtTIkSO1bdu2mD4cG2fMnz9fAwYMiM6HlZeXp9dffz26nOPSfMyePVuWZWny5MnRNo6PM2bOnCnLsmJenTp1ii5vdsfFIK4WLlxovF6v+e///m+zZcsWc/fdd5u0tDSzc+dOp4fWYixZssRMnz7dLF682Egyf/3rX2OWP/zwwyY9Pd0sXrzYbN682YwaNcp07tzZlJSURPtMmDDBnHvuuaagoMBs2LDBXH311eaiiy4ylZWV0T7f/va3TU5OjlmzZo1Zs2aNycnJMSNGjEjU10w63/rWt8zzzz9vPv74Y7Np0yZzww03mG7dupmjR49G+3BsnPHaa6+Zf/zjH2bbtm1m27Zt5v777zder9d8/PHHxhiOS3Oxdu1a06NHDzNgwABz9913R9s5Ps6YMWOG6devnykqKoq+9u/fH13e3I4LgS7OLrnkEjNhwoSYtt69e5upU6c6NKKW7eRAFw6HTadOnczDDz8cbSsrKzOBQMA89dRTxhhjjhw5Yrxer1m4cGG0z+7du43L5TJLly41xhizZcsWI8m899570T7vvvuukWQ+/fRTm79Vy7B//34jyaxcudIYw7Fpbtq2bWueffZZjkszUVpaai644AJTUFBghg4dGg10HB/nzJgxw1x00UX1LmuOx4VTrnFUXl6u9evXKz8/P6Y9Pz9fa9ascWhUZ5cvvvhCe/fujTkGPp9PQ4cOjR6D9evXq6KiIqZPly5dlJOTE+3z7rvvKhAI6NJLL432ueyyyxQIBDiWDVRcXCxJateunSSOTXMRCoW0cOFCHTt2THl5eRyXZuKuu+7SDTfcoOuuuy6mnePjrM8//1xdunRRdna2fvCDH2j79u2SmudxOeue5WqnAwcOKBQKKTMzM6Y9MzNTe/fudWhUZ5fI71zfMdi5c2e0T0pKitq2bVunT2T9vXv3qmPHjnW237FjR45lAxhjNGXKFH3zm99UTk6OJI6N0zZv3qy8vDyVlZWpdevW+utf/6q+fftG/6PBcXHOwoULtWHDBq1bt67OMv65cc6ll16qF154QRdeeKH27dunhx56SEOGDNEnn3zSLI8Lgc4GlmXFfDbG1GmDvRpzDE7uU19/jmXDTJw4UR999JHefvvtOss4Ns7o1auXNm3apCNHjmjx4sUaN26cVq5cGV3OcXHGrl27dPfdd+uNN96Q3+8/ZT+OT+Jdf/310ff9+/dXXl6ezj//fP3xj3/UZZddJql5HRdOucZR+/bt5Xa766Tq/fv310nxsEfkDqTTHYNOnTqpvLxchw8fPm2fffv21dn+V199xbH8GpMmTdJrr72m5cuXq2vXrtF2jo2zUlJS1LNnTw0ePFizZ8/WRRddpCeffJLj4rD169dr//79ys3Nlcfjkcfj0cqVK/W73/1OHo8n+ttxfJyXlpam/v376/PPP2+W/9wQ6OIoJSVFubm5KigoiGkvKCjQkCFDHBrV2SU7O1udOnWKOQbl5eVauXJl9Bjk5ubK6/XG9CkqKtLHH38c7ZOXl6fi4mKtXbs22uf9999XcXExx/IUjDGaOHGiXnnlFS1btkzZ2dkxyzk2zYsxRsFgkOPisGuvvVabN2/Wpk2boq/Bgwdr9OjR2rRpk8477zyOTzMRDAa1detWde7cuXn+c3NGt1Dga0WmLfmf//kfs2XLFjN58mSTlpZmduzY4fTQWozS0lKzceNGs3HjRiPJPP7442bjxo3RqWEefvhhEwgEzCuvvGI2b95sfvjDH9Z7K3nXrl3Nm2++aTZs2GCuueaaem8lHzBggHn33XfNu+++a/r3788t/qfxk5/8xAQCAbNixYqY2/yPHz8e7cOxcca0adPMqlWrzBdffGE++ugjc//99xuXy2XeeOMNYwzHpbmpfZerMRwfp9xzzz1mxYoVZvv27ea9994zI0aMMOnp6dH/nje340Kgs8HcuXNN9+7dTUpKihk0aFB02gbEx/Lly42kOq9x48YZY6puJ58xY4bp1KmT8fl85sorrzSbN2+O2caJEyfMxIkTTbt27UxqaqoZMWKEKSwsjOlz8OBBM3r0aJOenm7S09PN6NGjzeHDhxP0LZNPfcdEknn++eejfTg2zrj99tuj/07q0KGDufbaa6NhzhiOS3NzcqDj+DgjMq+c1+s1Xbp0Md/97nfNJ598El3e3I6LZYwxZ1h1BAAAQDPCNXQAAABJjkAHAACQ5Ah0AAAASY5ABwAAkOQIdAAAAEmOQAcAAJDkCHQAAABJjkAHAACQ5Ah0AJAAPXr00Jw5c5weBoAWikAHoMW57bbbNHLkSEnSVVddpcmTJyds3wsWLFCbNm3qtK9bt04//vGPEzYOAGcXj9MDAIBkUF5erpSUlEav36FDhziOBgBiUaED0GLddtttWrlypZ588klZliXLsrRjxw5J0pYtWzR8+HC1bt1amZmZGjNmjA4cOBBd96qrrtLEiRM1ZcoUtW/fXsOGDZMkPf744+rfv7/S0tKUlZWl//qv/9LRo0clSStWrNB//ud/qri4OLq/mTNnSqp7yrWwsFA33XSTWrdurYyMDN18883at29fdPnMmTM1cOBAvfjii+rRo4cCgYB+8IMfqLS01N4fDUBSItABaLGefPJJ5eXl6Uc/+pGKiopUVFSkrKwsFRUVaejQoRo4cKA++OADLV26VPv27dPNN98cs/4f//hHeTwevfPOO3r66aclSS6XS7/73e/08ccf649//KOWLVum++67T5I0ZMgQzZkzRxkZGdH93XvvvXXGZYzRyJEjdejQIa1cuVIFBQX697//rVGjRsX0+/e//61XX31Vf//73/X3v/9dK1eu1MMPP2zTrwUgmXHKFUCLFQgElJKSolatWqlTp07R9vnz52vQoEH69a9/HW177rnnlJWVpc8++0wXXnihJKlnz5569NFHY7ZZ+3q87OxsPfjgg/rJT36iefPmKSUlRYFAQJZlxezvZG+++aY++ugjffHFF8rKypIkvfjii+rXr5/WrVuniy++WJIUDoe1YMECpaenS5LGjBmjt956S7/61a+a9sMAaHGo0AE466xfv17Lly9X69ato6/evXtLqqqKRQwePLjOusuXL9ewYcN07rnnKj09XWPHjtXBgwd17NixBu9/69atysrKioY5Serbt6/atGmjrVu3Rtt69OgRDXOS1LlzZ+3fv/+MviuAswMVOgBnnXA4rBtvvFGPPPJInWWdO3eOvk9LS4tZtnPnTg0fPlwTJkzQgw8+qHbt2untt9/WHXfcoYqKigbv3xgjy7K+tt3r9cYstyxL4XC4wfsBcPYg0AFo0VJSUhQKhWLaBg0apMWLF6tHjx7yeBr+r8EPPvhAlZWV+u1vfyuXq+oEx5///Oev3d/J+vbtq8LCQu3atStapduyZYuKi4vVp0+fBo8HACI45QqgRevRo4fef/997dixQwcOHFA4HNZdd92lQ4cO6Yc//KHWrl2r7du364033tDtt99+2jB2/vnnq7KyUr///e+1fft2vfjii3rqqafq7O/o0aN66623dODAAR0/frzOdq677joNGDBAo0eP1oYNG7R27VqNHTtWQ4cOrfc0LwB8HQIdgBbt3nvvldvtVt++fdWhQwcVFhaqS5cueueddxQKhfStb31LOTk5uvvuuxUIBKKVt/oMHDhQjz/+uB555BHl5OToT3/6k2bPnh3TZ8iQIZowYYJGjRqlDh061LmpQqo6dfrqq6+qbdu2uvLKK3XdddfpvPPO06JFi+L+/QGcHSxjjHF6EAAAAGg8KnQAAABJjkAHAACQ5Ah0AAAASY5ABwAAkOQIdAAAAEmOQAcAAJDkCHQAAABJjkAHAACQ5Ah0AAAASY5ABwAAkOQIdAAAAEmOQAcAAJDk/n/ilhMH3WlvTgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w, b, costs = fit_logistic_regression(\n",
    "    X_train_scaled, y_train.values, lr=0.1, n_iter=5000\n",
    ")\n",
    "\n",
    "plt.plot(np.arange(0, len(costs))*100, costs)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Training loss')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ecdbb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (from scratch): 0.9825\n",
      "\n",
      "Learned coefficients (betas):\n",
      "  beta_0: -0.4403\n",
      "  beta_1: -0.4094\n",
      "  beta_2: -0.4132\n",
      "  beta_3: -0.5128\n",
      "  beta_4: -0.2688\n",
      "  beta_5: 0.5358\n",
      "  beta_6: -0.9503\n",
      "  beta_7: -1.0277\n",
      "  beta_8: 0.2798\n",
      "  beta_9: 0.4411\n",
      "  beta_10: -1.6313\n",
      "  beta_11: 0.0197\n",
      "  beta_12: -0.9842\n",
      "  beta_13: -1.1468\n",
      "  beta_14: -0.3618\n",
      "  beta_15: 1.1791\n",
      "  beta_16: 0.0749\n",
      "  beta_17: -0.4285\n",
      "  beta_18: 0.3838\n",
      "  beta_19: 0.8358\n",
      "  beta_20: -1.3487\n",
      "  beta_21: -1.3953\n",
      "  beta_22: -1.0982\n",
      "  beta_23: -1.2824\n",
      "  beta_24: -0.8488\n",
      "  beta_25: -0.1039\n",
      "  beta_26: -1.1710\n",
      "  beta_27: -1.0961\n",
      "  beta_28: -1.2386\n",
      "  beta_29: -0.6249\n",
      "  bias (intercept): 0.1609\n"
     ]
    }
   ],
   "source": [
    "def predict(X, w, b, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict binary labels using learned weights and bias.\n",
    "    Inputs:\n",
    "        X : Feature matrix (m, n)\n",
    "        w : Weights (n,)\n",
    "        b : Bias (scalar)\n",
    "        threshold : Classification threshold (default = 0.5)\n",
    "    Returns:\n",
    "        Binary predictions (0 or 1)\n",
    "    \"\"\"\n",
    "    probs = sigmoid(X @ w + b)\n",
    "    return (probs >= threshold).astype(int)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_test = predict(X_test_scaled, w, b)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy_scratch = np.mean(y_pred_test == y_test.values)\n",
    "print(f'Accuracy (from scratch): {accuracy_scratch:.4f}')\n",
    "\n",
    "# Print learned coefficients\n",
    "print('\\nLearned coefficients (betas):')\n",
    "for i, coef in enumerate(w):\n",
    "    print(f'  beta_{i}: {coef:.4f}')\n",
    "print(f'  bias (intercept): {b:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a54cbf",
   "metadata": {},
   "source": [
    "## Logistic Regression with scikit‑learn\n",
    "\n",
    "### Switching to Scikit-Learn's Logistic Regression\n",
    "\n",
    "With the mathematical foundations and our from-scratch routine in place, we will now switch to a **production-ready implementation** provided by `scikit-learn`.\n",
    "\n",
    "This serves two complementary goals:\n",
    "\n",
    "- **Speed & convenience** – The library’s C-optimised solvers yield the maximum-likelihood solution in milliseconds and expose a rich API for cross-validation, pipelines, and hyper-parameter tuning.\n",
    "\n",
    "- **Validation of our handcrafted model** – By comparing predictions and performance metrics against the NumPy implementation, we can confirm that both reach essentially the same optimum.\n",
    "\n",
    "---\n",
    "\n",
    "### Road-map\n",
    "\n",
    "| Step                          | Purpose                                                                                                  |\n",
    "|------------------------------|----------------------------------------------------------------------------------------------------------|\n",
    "| **Instantiate `LogisticRegression`** | Choose a solver (`lbfgs` is default), set `max_iter`, and decide on regularisation via the inverse penalty strength $C $. |\n",
    "| **Fit on the training set**  | The model internally standardises nothing, so we feed it the already scaled features.                   |\n",
    "| **Generate predictions**     | We will obtain both class labels (`predict`) and probabilities (`predict_proba`) to enable threshold-agnostic evaluation. |\n",
    "| **Evaluate performance**     | Compute accuracy, confusion matrix, ROC-AUC, log-loss, and optionally precision-recall metrics — tools well suited to class-imbalance diagnostics. |\n",
    "| **Inspect coefficients**     | Extract $\\hat{\\beta} $ values, convert them to odds ratios $e^{\\beta_j} $, and discuss which predictors exert the strongest multiplicative effect on the class odds. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c09eef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (scikit-learn): 0.9912\n",
      "\n",
      "\n",
      "Learned coefficients (betas):\n",
      "  beta_0: -0.3939\n",
      "  beta_1: -0.3607\n",
      "  beta_2: -0.3787\n",
      "  beta_3: -0.4202\n",
      "  beta_4: -0.2093\n",
      "  beta_5: 0.3420\n",
      "  beta_6: -0.7558\n",
      "  beta_7: -0.8115\n",
      "  beta_8: 0.1296\n",
      "  beta_9: 0.3603\n",
      "  beta_10: -1.1783\n",
      "  beta_11: 0.0375\n",
      "  beta_12: -0.7008\n",
      "  beta_13: -0.8346\n",
      "  beta_14: -0.2879\n",
      "  beta_15: 0.8301\n",
      "  beta_16: 0.1092\n",
      "  beta_17: -0.3279\n",
      "  beta_18: 0.2822\n",
      "  beta_19: 0.5922\n",
      "  beta_20: -1.0453\n",
      "  beta_21: -1.0836\n",
      "  beta_22: -0.8613\n",
      "  beta_23: -0.9758\n",
      "  beta_24: -0.5900\n",
      "  beta_25: -0.0990\n",
      "  beta_26: -0.8642\n",
      "  beta_27: -0.8212\n",
      "  beta_28: -0.9118\n",
      "  beta_29: -0.4852\n",
      "  bias (intercept): 0.2962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Train logistic regression model\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_sklearn = clf.predict(X_test_scaled)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "\n",
    "# Print results\n",
    "print(f'Accuracy (scikit-learn): {accuracy_sklearn:.4f}\\n')\n",
    "\n",
    "# Print coefficients and intercept\n",
    "print('\\nLearned coefficients (betas):')\n",
    "for i, coef in enumerate(clf.coef_[0]):\n",
    "    print(f'  beta_{i}: {coef:.4f}')\n",
    "print(f'  bias (intercept): {clf.intercept_[0]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a19da",
   "metadata": {},
   "source": [
    "## Results & Comparison <a id=\"comparison\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fdda79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From‑scratch accuracy: 0.9825\n",
      "scikit‑learn accuracy: 0.9912\n",
      "Difference: 0.0088\n"
     ]
    }
   ],
   "source": [
    "print(f'From‑scratch accuracy: {accuracy_scratch:.4f}')\n",
    "print(f'scikit‑learn accuracy: {accuracy_sklearn:.4f}')\n",
    "print(f'Difference: {accuracy_sklearn - accuracy_scratch:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9152d5-1a38-476b-8394-e79b02e5d12b",
   "metadata": {},
   "source": [
    "### Interpreting the Accuracy Comparison\n",
    "\n",
    "| Implementation                  | Test-set Accuracy |\n",
    "|----------------------------------|-------------------|\n",
    "| From-scratch (NumPy + gradient descent) | 0.9825            |\n",
    "| scikit-learn (`LogisticRegression`)     | 0.9912            |\n",
    "| **Absolute gap**                        | **0.0088** (≈ 0.9 percentage points) |\n",
    "\n",
    "---\n",
    "\n",
    "### What This Tells Us\n",
    "\n",
    "- **Both models capture essentially the same signal.**  \n",
    "  An accuracy in the high 98–99% range confirms that the underlying decision boundary learned from the data is nearly identical.\n",
    "\n",
    "- **The small gap arises from solver and regularisation differences.**\n",
    "\n",
    "  - **Solver:** Our hand-coded version uses plain batch gradient descent, whereas scikit-learn defaults to the **LBFGS optimiser**—a quasi-Newton method that usually converges to a slightly better optimum.\n",
    "\n",
    "  - **Regularisation:** `LogisticRegression` applies **L2 penalty by default** (inverse strength $C = 1 $). Our manual model had **no penalty term**, so coefficients are unconstrained. A mild penalty can improve generalisation and nudge the likelihood to a marginally higher maximum.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Takeaway\n",
    "\n",
    "A ~0.9 percentage point improvement is statistically minor in many contexts, yet it showcases the benefit of a **production-grade optimiser** with **sensible defaults**.\n",
    "\n",
    "The close agreement **validates the maths** in the from-scratch implementation—an important sanity check before trusting more complex, custom code.\n",
    "\n",
    "For deployment, you would typically rely on the **scikit-learn model** (or a more advanced pipeline), because it is:\n",
    "\n",
    "- Faster to train  \n",
    "- Easier to cross-validate  \n",
    "- Provides built-in support for regularisation and probability calibration\n",
    "\n",
    "---\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "The result demonstrates that a **carefully written bare-bones implementation** can rival a library solution, while the library still offers **marginal gains** and **far greater convenience** for production workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f8aec-e9db-406b-9b00-5f2050eb33f2",
   "metadata": {},
   "source": [
    "## Practical Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eda1e2-9f6e-4e3b-ba01-4fa0e5b6a2c4",
   "metadata": {},
   "source": [
    "### Handling Class Imbalance\n",
    "\n",
    "Real-world datasets often contain far fewer positive cases than negatives (e.g., fraud ≪ non-fraud, churn ≪ retained).  \n",
    "If left untreated, a classifier can achieve deceptively high accuracy by **always predicting the majority class**.\n",
    "\n",
    "Below are proven tactics—organised from **simplest to most sophisticated**—to mitigate that bias.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Approaches to Class Imbalance\n",
    "\n",
    "| Approach                     | Core Idea                                                                                  | Pros                                                                 | Cons / Caveats                                                              |\n",
    "|-----------------------------|---------------------------------------------------------------------------------------------|----------------------------------------------------------------------|------------------------------------------------------------------------------|\n",
    "| **Resample the data**       | Under-sample the majority class or over-sample the minority (e.g., random duplication, SMOTE). | Model remains unchanged; easy to reason about.                        | Under-sampling discards information; naïve over-sampling can overfit.       |\n",
    "| **Weighted (cost-sensitive) loss** | Assign a larger weight $w $ to minority observations in the log-likelihood:<br> $J(\\boldsymbol{\\beta}) = -\\sum_{i=1}^m w_i \\left[y_i \\log p_i + (1 - y_i)\\log(1 - p_i)\\right] $<br>Common choice: $w_i = \\frac{m}{2 \\cdot \\text{count}(y_i)} $ | Uses full data; integrates with gradient-based solvers.              | Must tune or cross-validate weights; extreme weights can cause instability. |\n",
    "| **Adjust the decision threshold** | Default threshold 0.5 maximises accuracy only for balanced data. Choose $\\tau $ that maximises $F_\\beta $ or minimises expected cost.<br> Predict “positive” if $p_i \\geq \\tau $. | Simple post-processing; no model retrain needed.                      | Only changes decision rule, not model boundary.                            |\n",
    "| **Custom evaluation metrics** | Optimise for precision-recall AUC, $F_1 $, MCC, or expected cost instead of accuracy.     | Forces hyper-parameter search to respect minority performance.        | Requires domain knowledge to select appropriate metrics.                    |\n",
    "| **Hybrid or ensemble methods** | Combine logistic regression with tree-based models on balanced bootstraps; or stack predictions and weight them. | Captures linear interpretability + nonlinear patterns; boosts recall. | More complex; interpretability of the ensemble may decrease.                |\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Intuition Behind Weighting\n",
    "\n",
    "For class weights $w_0 $ and $w_1 $ assigned to negatives and positives respectively, the **weighted gradient** becomes:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\boldsymbol{\\beta}} J = \\sum_{i=1}^{m} w_{y_i} (p_i - y_i) \\mathbf{x}_i, \\quad p_i = \\sigma(\\mathbf{x}_i^\\top \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "A larger $w_1 $ amplifies the error term for minority instances, steering the optimiser to reduce **false negatives** at the expense of some **false positives**—exactly what you want when a missed positive is costlier.\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Which Tactic?\n",
    "\n",
    "- **Mild imbalance (≤ 4:1)**  \n",
    "  Weighted loss or threshold tuning alone often suffices.\n",
    "\n",
    "- **Moderate to severe imbalance (5:1 – 100:1)**  \n",
    "  Combine weighted loss with intelligent over/under-sampling (e.g., SMOTE + Tomek links).\n",
    "\n",
    "- **Extreme imbalance (≫ 100:1)**  \n",
    "  Consider anomaly detection framing or hybrid ensembles; supplement evaluation with business-cost analysis.\n",
    "\n",
    "---\n",
    "\n",
    "By consciously addressing imbalance, the logistic-regression pipeline remains both **predictive and fair**, yielding **calibrated probabilities** that truly reflect minority-class risk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05342f2-be3e-4f1f-af97-ee1692157bf2",
   "metadata": {},
   "source": [
    "### Feature Scaling — Why It Matters and How to Do It\n",
    "\n",
    "**Feature scaling** is the practice of transforming predictors so they share a common numerical scale.  \n",
    "Although logistic regression is scale-invariant *in theory*—the decision boundary depends only on sign changes of $ \\beta_j x_j $—in practice, scaling has far-reaching effects on **optimisation**, **regularisation**, and **interpretability**.\n",
    "\n",
    "---\n",
    "\n",
    "### Impact of (Not) Scaling\n",
    "\n",
    "| Aspect                   | Impact                                                                                                                                |\n",
    "|--------------------------|---------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Gradient-based solvers** | The loss surface becomes ill-conditioned when features are on drastically different scales. Step sizes that suit “age in years” may cause divergence for “income in dollars”. Scaling makes the Hessian closer to spherical, so gradient descent or LBFGS converges in fewer iterations. |\n",
    "| **Regularisation**         | $ L_1 $ and $ L_2 $ penalties shrink $ \\|\\boldsymbol{\\beta}\\| $. Without scaling, variables with large units get over-penalised, biasing feature selection. With scaling, each predictor pays a comparable regularisation cost. |\n",
    "| **Coefficient comparison** | A raw coefficient reflects a one-unit change in the original units (e.g., \\$1). After standardisation, it reflects a one-standard-deviation change, enabling direct ranking of variable importance. |\n",
    "| **Numeric stability**      | Extremely large or tiny values can trigger floating-point overflow in the sigmoid $ \\sigma(z) $. Scaling brings $ z = \\mathbf{x}^\\top \\boldsymbol{\\beta} $ into a numerically safe range. |\n",
    "\n",
    "---\n",
    "\n",
    "### Common Scaling Strategies\n",
    "\n",
    "| Method                | Formula                                      | When to Use                                                                 |\n",
    "|-----------------------|----------------------------------------------|------------------------------------------------------------------------------|\n",
    "| **Z-score standardisation** | $ x^* = \\frac{x - \\mu}{\\sigma} $               | Default choice for gradient-based models; mean 0, SD 1.                      |\n",
    "| **Min–max scaling**        | $ x^* = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}} $ | Needed when features must lie in $[0, 1]$, e.g., neural nets with sigmoid/tanh. |\n",
    "| **Robust scaling**         | $ x^* = \\frac{x - \\text{median}}{\\text{IQR}} $      | Resistant to heavy-tailed outliers.                                         |\n",
    "| **Log / power transforms** | $ x^* = \\log(x + 1) $ or Box–Cox                   | For strictly positive, highly skewed distributions (e.g., income).          |\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Fit the scaler on the training set only**  \n",
    "  Compute $ \\mu $ and $ \\sigma $ (or other parameters) on $ X_{\\text{train}} $ to avoid leaking information from the test set.\n",
    "\n",
    "- **Apply the same transformation to all future data**  \n",
    "  Store the fitted parameters and reuse them during inference or deployment.\n",
    "\n",
    "- **Combine scaling with pipelines**  \n",
    "  In scikit-learn, wrap `StandardScaler()` and `LogisticRegression()` in a `Pipeline` to guarantee consistent preprocessing during cross-validation.\n",
    "\n",
    "- **Check for categorical dummies**  \n",
    "  One-hot-encoded binaries $\\{0, 1\\}$ are already on a comparable scale—scaling them is harmless but not required.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "Scaling is **not merely cosmetic**; it directly improves:\n",
    "\n",
    "- Convergence speed  \n",
    "- Regularisation fairness  \n",
    "- Numerical robustness  \n",
    "- Interpretability  \n",
    "\n",
    "Always include feature scaling as a **first-class step** in your preprocessing pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a13bfe-c1e8-4b6a-83d2-45d680a002eb",
   "metadata": {},
   "source": [
    "### Regularisation — Taming Over-fitting and Selecting Features\n",
    "\n",
    "Logistic regression maximises the log-likelihood, but nothing in that criterion penalises overly complex models.  \n",
    "**Regularisation** adds a complexity term to the objective, discouraging extreme coefficient values that over-fit noise.\n",
    "\n",
    "---\n",
    "\n",
    "### 1 Penalised Objective\n",
    "\n",
    "$$\n",
    "-\\sum_{i=1}^m \\left[ y_i \\log p_i + (1 - y_i) \\log(1 - p_i) \\right] \\quad \\text{(Negative log-likelihood)} \\\\\n",
    "+ \\quad \\lambda \\, \\Omega$boldsymbol{\\beta}) \\quad \\text{(Penalty)} \\quad \\longrightarrow \\min_{\\boldsymbol{\\beta}}, \\quad p_i = \\sigma$mathbf{x}_i^\\top \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "- $ \\lambda \\geq 0 $ controls the **strength** of the penalty.\n",
    "- $ \\Omega$boldsymbol{\\beta}) $ defines which kind of **shrinkage** we impose.\n",
    "\n",
    "---\n",
    "\n",
    "### 2 Common Penalty Types\n",
    "\n",
    "| Penalty         | Form $ \\Omega$boldsymbol{\\beta}) $                          | Behaviour                                                                 | Typical Use-Case                                               |\n",
    "|-----------------|----------------------------------------------------------------|---------------------------------------------------------------------------|----------------------------------------------------------------|\n",
    "| **L2 (Ridge)**  | $ \\frac{1}{2} \\sum_{j=1}^k \\beta_j^2 $                       | Shrinks coefficients continuously; keeps all features but smaller.       | Multicollinearity control, smooth shrinkage when all features carry signal. |\n",
    "| **L1 (Lasso)**  | $ \\sum_{j=1}^{k} |\\beta_j| $                                 | Encourages sparsity; eliminates irrelevant features.                      | Feature selection, interpretability.                          |\n",
    "| **Elastic Net** | $ \\alpha L_1 + (1 - \\alpha) L_2 $                            | Blend of L1 and L2; balances sparsity with stability under collinearity. | Genomics, text, or $ k \\gg m $ problems.                    |\n",
    "\n",
    "> **Note:** In scikit-learn, regularisation strength is parameterised as $ C = \\frac{1}{\\lambda} $.  \n",
    "Smaller $ C $ ⇒ stronger penalty.\n",
    "\n",
    "---\n",
    "\n",
    "### 3 Intuitive Effect\n",
    "\n",
    "Penalised coefficients are equivalent to posterior modes under zero-centred priors:\n",
    "\n",
    "| Penalty | Bayesian Prior               |\n",
    "|---------|------------------------------|\n",
    "| L2      | $ \\beta_j \\sim \\mathcal{N}(0, \\tau^2) $     |\n",
    "| L1      | $ \\beta_j \\sim \\text{Laplace}(0, b) $       |\n",
    "\n",
    "Thus, regularisation expresses a **prior belief** that large weights are unlikely unless the data provide strong evidence.\n",
    "\n",
    "---\n",
    "\n",
    "### 4 Choosing $ \\lambda $ (or $ C $)\n",
    "\n",
    "- **Cross-validation**  \n",
    "  Split training data; fit across a **logarithmic grid** of $ \\lambda $; select value that maximises ROC-AUC or minimises log-loss.\n",
    "\n",
    "- **Information criteria**  \n",
    "  AIC/BIC can be adapted to penalised likelihoods.\n",
    "\n",
    "- **Domain knowledge**  \n",
    "  Regulatory models (e.g., credit scoring) may impose limits on coefficient magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "### 5 Interplay with Feature Scaling\n",
    "\n",
    "Penalties act on **absolute coefficient values**.\n",
    "\n",
    "- If predictors are **unscaled**, features measured in large units appear artificially “small” and may dodge the penalty.\n",
    "- **Always scale** features before applying regularisation to ensure **equitable shrinkage**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6 Practical Guidelines\n",
    "\n",
    "- **Start with L2** for balanced, low-dimensional problems — it typically improves generalisation with minimal interpretability cost.\n",
    "- **Try L1 or Elastic Net** when you expect **many irrelevant or collinear predictors**. Inspect the sparsity pattern for insights.\n",
    "- **Monitor validation loss vs. $ \\lambda $** — an “elbow” often marks the sweet spot between **variance and bias**.\n",
    "- Remember: **too much regularisation = under-fitting**. Aim for just enough shrinkage to curb noise without erasing signal.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Takeaway\n",
    "\n",
    "By incorporating regularisation into logistic regression, you obtain models that:\n",
    "\n",
    "- Generalise better  \n",
    "- Remain numerically stable  \n",
    "- Optionally reveal the **most informative features**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fffb7fc-19c4-4742-b243-cbf49cfd00f7",
   "metadata": {},
   "source": [
    "### Model Interpretation — Turning Coefficients into Actionable Insights\n",
    "\n",
    "Logistic regression is prized for the clarity with which it links predictors to class probabilities.  \n",
    "Once the model is trained, interpretation proceeds along several complementary axes.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretive Lenses\n",
    "\n",
    "| Lens                         | What You Examine                                                      | How to Read It                                                                                                 |\n",
    "|------------------------------|------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|\n",
    "| **Sign of a coefficient**    | $ \\beta_j $                                                         | $ > 0 \\Rightarrow $ predictor increases log-odds of the positive class; $ < 0 \\Rightarrow $ decreases them. |\n",
    "| **Magnitude on log-odds scale** | $ \\beta_j $                                                         | Indicates how strongly $ x_j $ influences the log-odds; useful for ranking importance (if scaled).            |\n",
    "| **Odds ratio**               | $ e^{\\beta_j} $                                                     | Multiplicative change in odds for a one-unit (or one-SD) increase in $ x_j $. Example: $ e^{0.7} \\approx 2.0 $ doubles the odds. |\n",
    "| **Confidence interval**      | $ \\beta_j \\pm 1.96 \\cdot SE(\\beta_j) $                              | If the interval crosses 0, the effect is not statistically distinguishable from noise at the 5% level.         |\n",
    "| **Average Marginal Effect (AME)** | $ \\frac{1}{m} \\sum_{i=1}^m p_i(1 - p_i)x_{ij} $                      | Expected change in **probability**, not log-odds, when $ x_j $ rises by one unit; more intuitive for stakeholders. |\n",
    "| **Partial dependence**       | $ \\hat{p}(x_j, \\text{rest}) $ plotted across a range               | Visualises how predicted probability changes with one feature holding others fixed.                            |\n",
    "| **Interaction terms**        | $ \\beta_{jk} x_j x_k $                                              | A significant $ \\beta_{jk} $ means the effect of $ x_j $ depends on $ x_k $; interpret with plots or marginal effects. |\n",
    "| **Global vs. local explanation** | Global: coefficients, AME<br>Local: $ \\Delta \\log \\text{odds} = \\boldsymbol{\\beta}^\\top$mathbf{x} - \\mathbf{x}_{\\text{ref}}) $ | Global tells “which features matter overall,” local explains “why this specific prediction is high/low.”        |\n",
    "\n",
    "---\n",
    "\n",
    "### Interpreting the Intercept\n",
    "\n",
    "The intercept $ \\beta_0 $ equals the **log-odds when all predictors are zero** (or at their means if features were centred).  \n",
    "Exponentiating gives the **baseline odds**: $ e^{\\beta_0} $.  \n",
    "This provides context for how much each feature shifts the model away from the base rate.\n",
    "\n",
    "---\n",
    "\n",
    "### Regularisation Caveat\n",
    "\n",
    "Shrinking coefficients alters their magnitudes.  \n",
    "**Odds ratios from a heavily penalised model** reflect both **signal and the bias** introduced by regularisation.\n",
    "\n",
    "> When interpretability is paramount, consider:\n",
    "- Using a **milder penalty**, or  \n",
    "- Presenting **unpenalised coefficients** alongside penalised predictions\n",
    "\n",
    "---\n",
    "\n",
    "### Communicating Results\n",
    "\n",
    "- **Translate odds ratios to plain language**  \n",
    "  > “A one-SD increase in tumour radius multiplies the odds of malignancy by 2.3×, holding all other variables constant.”\n",
    "\n",
    "- **Quantify uncertainty**  \n",
    "  Report **95% confidence intervals** or **p-values** to show which effects are statistically robust.\n",
    "\n",
    "- **Show practical impact**  \n",
    "  Convert log-odds shifts into **absolute probability changes** at meaningful baselines (e.g., from 5% to 9%).\n",
    "\n",
    "---\n",
    "\n",
    "### Final Takeaway\n",
    "\n",
    "By combining:\n",
    "- **Coefficient signs**\n",
    "- **Odds ratios**\n",
    "- **Marginal effects**\n",
    "- **Confidence intervals**\n",
    "\n",
    "Logistic regression offers a **transparent bridge** between statistical modelling and decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa33d81-4917-40b0-8154-05cb47676acf",
   "metadata": {},
   "source": [
    "### Example for Interpreting the coefficients\n",
    "\n",
    "\n",
    "| Coefficient            | Value    | On the Log-Odds Scale                                      | As an Odds Ratio                  | Practical Meaning*                                                                 |\n",
    "|------------------------|----------|-------------------------------------------------------------|------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| **Intercept** $( \\beta_0 $)    | 0.2962   | Baseline log-odds when all predictors are zero (or at their means, if standardised) | $ e^{0.2962} \\approx 1.34 $      | Baseline odds of the positive class are **1.34 : 1** → a baseline probability of ≈ **57%** |\n",
    "| **Feature 29** $( \\beta_{29} $) | −0.4852  | Change in log-odds for a one-unit ↑ in $ x_{29} $         | $ e^{-0.4852} \\approx 0.62 $     | Multiplies the odds by **0.62×** (i.e., reduces them by **38%**)                  |\n",
    "\n",
    "\\* *Assuming the model was fit after z-score standardisation of the predictors.*\n",
    "\n",
    "---\n",
    "\n",
    "#### **Intercept (Baseline Risk)**\n",
    "\n",
    "When every feature is at its reference level (zero after standardisation), the model’s log-odds are **0.2962**.\n",
    "\n",
    "**Converting log-odds to probability:**\n",
    "\n",
    "$$\n",
    "p_0 = \\frac{e^{0.2962}}{1 + e^{0.2962}} \\approx 0.57\n",
    "$$\n",
    "\n",
    "So, even before considering any features, the event is **slightly more likely than not** (57% chance).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Feature 29**\n",
    "\n",
    "A **one-unit increase** in $x_{29}$(i.e., one standard deviation) lowers the log-odds by **0.4852**.\n",
    "\n",
    "**Odds ratio:**\n",
    "\n",
    "$$\n",
    "e^{-0.4852} \\approx 0.62\n",
    "$$\n",
    "\n",
    "This means the odds of the positive outcome are **38% lower** for each such increase.\n",
    "\n",
    "---\n",
    "\n",
    "**Example at the baseline:**\n",
    "\n",
    "Let the initial odds be 1.34 (from baseline probability):\n",
    "\n",
    "$$\n",
    "\\text{new odds} = 1.34 \\times 0.62 \\approx 0.83\n",
    "$$\n",
    "$$\n",
    "\\text{new probability} = \\frac{0.83}{1 + 0.83} \\approx 0.45\n",
    "$$\n",
    "\n",
    "Thus, raising $x_{29}$by one standard deviation **drops the predicted probability from 57% to about 45%**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Interpretation in Plain Language**\n",
    "\n",
    "> Holding all other variables constant, **feature 29 has a protective (negative) effect**:  \n",
    "> Increasing it by one standard deviation **cuts the odds** of the positive class by roughly a third, reducing the predicted probability from **a bit over one-half to just under one-half**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97b8f2a3-9534-45f5-a0b4-49a276a1c421",
   "metadata": {},
   "source": [
    "### How the Coefficients Shape the Logistic S-Curve\n",
    "\n",
    "![How the Coefficients Shape the Logistic S-Curve](images/How%20the%20Coefficients%20Shape%20the%20Logistic%20S-Curve.png)\n",
    "\n",
    "The logistic model with a single predictor can be written as:\n",
    "\n",
    "$$\n",
    "p(x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad z = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "The picture above visualises three key geometric facts:\n",
    "\n",
    "---\n",
    "\n",
    "### Geometric Interpretation of Logistic Coefficients\n",
    "\n",
    "| Geometric Feature on the Plot                | Algebraic Expression                              | Interpretation                                                                 |\n",
    "|----------------------------------------------|----------------------------------------------------|---------------------------------------------------------------------------------|\n",
    "| **Horizontal shift of the curve** (grey line) | $x = -\\frac{\\beta_0}{\\beta_1}$                  | The inflection point where $p = 0.5$. The intercept $\\beta_0$ shifts the S-curve left or right. |\n",
    "| **Steepness** (slope of dashed tangent)       | $\\frac{dp}{dx} \\bigg|_{p=0.5} = \\frac{\\beta_1}{4}$ | The derivative of the sigmoid at the midpoint; higher $\\beta_1$ = steeper curve. |\n",
    "| **Width of transition band** (red arrow)      | Central decile span ≈ $\\frac{4.394}{\\beta_1}$     | Range of $x$ where $p(x) \\in [0.1, 0.9]$; inversely proportional to steepness. |\n",
    "| **Direction of change**                       | Sign of $\\beta_1$                                 | If $\\beta_1 > 0$, $p(x)$ increases with $x$; if $\\beta_1 < 0$, $p(x)$ decreases. |\n",
    "\n",
    "---\n",
    "\n",
    "### Putting It Together\n",
    "\n",
    "- **Intercept $\\beta_0$**  \n",
    "  Sets the **horizontal position** of the curve but **does not affect its steepness**.  \n",
    "  It is the log-odds when $x = 0$.\n",
    "\n",
    "- **Slope $\\beta_1$**  \n",
    "  Governs the **scale of the x-axis**:\n",
    "  - **Steeper curve** ⇒ model is more **certain** (probabilities quickly approach 0 or 1).\n",
    "  - **Flatter curve** ⇒ model is more **uncertain** (probabilities stay near 0.5 over a wider range).\n",
    "  - **Negative $\\beta_1$** ⇒ the S-curve **flips**, becoming a decreasing function of $x$.\n",
    "\n",
    "---\n",
    "\n",
    "### Extension to Multivariate Case\n",
    "\n",
    "These geometric insights extend to multivariate logistic regression:\n",
    "\n",
    "- Each coefficient $\\beta_j$ **tilts the surface** along feature $x_j$\n",
    "- The intercept vector $\\beta_0$ **shifts the entire decision surface**\n",
    "\n",
    "Visualising the one-dimensional slice helps clarify **how changes in coefficient magnitude or sign reshape the probability landscape**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f509da",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Over the course of this notebook we have:\n",
    "\n",
    "- **Motivated logistic regression** as the canonical model for binary outcomes, contrasting it with linear regression and highlighting the probabilistic interpretation of the sigmoid link.\n",
    "\n",
    "- **Built the algorithm from scratch**—deriving the likelihood, gradients, and a gradient-descent optimiser—thereby demystifying what high-level libraries do under the hood.\n",
    "\n",
    "- **Validated our hand-rolled model** against `scikit-learn`, discovering that a production-grade solver with mild L2 regularisation can squeeze out an extra sliver of accuracy while confirming our implementation’s correctness.\n",
    "\n",
    "- **Added practical engineering layers**:\n",
    "  - **Feature scaling** for faster convergence, fair regularisation, and interpretable coefficients.\n",
    "  - **Class-imbalance strategies** such as weighted loss, resampling, and threshold tuning.\n",
    "  - **Regularisation** (L1, L2, Elastic Net) to control over-fitting and encourage sparsity.\n",
    "\n",
    "- **Explored model interpretation**—signs, magnitudes, odds ratios, marginal effects—culminating in a **geometric view** of how $ \\beta_0$ and $ \\beta_1$ shift and stretch the S-curve.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Logistic regression remains a first-stop classifier**: simple, fast, interpretable, yet surprisingly powerful when properly pre-processed and regularised.\n",
    "\n",
    "- **Understanding the mathematics** (likelihood, gradient, penalty) equips you to:\n",
    "  - Diagnose convergence issues  \n",
    "  - Justify weighting schemes  \n",
    "  - Communicate effect sizes in plain language\n",
    "\n",
    "- **Scaling, regularisation, and imbalance handling** are not afterthoughts—they are **integral** to building reliable, generalisable models.\n",
    "\n",
    "---\n",
    "\n",
    "Armed with these foundations, you can confidently:\n",
    "\n",
    "- Apply logistic regression to real-world problems  \n",
    "- Benchmark more complex algorithms  \n",
    "- Explain your findings to both technical and non-technical stakeholders\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
